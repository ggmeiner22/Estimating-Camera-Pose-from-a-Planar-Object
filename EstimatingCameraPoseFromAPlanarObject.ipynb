{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggmeiner22/Estimating-Camera-Pose-from-a-Planar-Object/blob/main/EstimatingCameraPoseFromAPlanarObject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal\n",
        "\n",
        "Given a calibrated camera (i.e., known intrinsics **K**) and a single image of a planar object, estimate the camera **pose** (i.e., extrinsics **R**, **t**).\n",
        "\n",
        "I will:\n",
        "\n",
        "1. build a Gradio UI to click 2D features and manage point order,\n",
        "2. estimate pose **from a homography** (explicit derivation), and\n",
        "3. estimate pose using **OpenCV** functions,\n",
        "4. compare the two (short notes only)."
      ],
      "metadata": {
        "id": "EpbqkAzHIM5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs dependencies"
      ],
      "metadata": {
        "id": "CWn5jpoaI94w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "B2u8_q2_IJ7l"
      },
      "outputs": [],
      "source": [
        "%pip -q install opencv-python numpy gradio matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "SqNF6-vkLfzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, io, math, os\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "from matplotlib.figure import Figure\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import plotly.graph_objects as go         # Interactive plotting (3D/2D)"
      ],
      "metadata": {
        "id": "yyqTxE69LgHW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Camera Class"
      ],
      "metadata": {
        "id": "oapejCBh0SXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cam:\n",
        "    \"\"\"\n",
        "    Camera-related utilities for calibration and pose handling.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def rodrigues_to_R(rvec: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert a Rodrigues rotation vector to a 3x3 rotation matrix.\n",
        "\n",
        "        Args:\n",
        "            rvec: Rotation vector (3x1 or 1x3), Rodrigues representation.\n",
        "\n",
        "        Returns:\n",
        "            R: Rotation matrix (3x3).\n",
        "        \"\"\"\n",
        "        R, _ = cv.Rodrigues(rvec)\n",
        "        return R\n",
        "\n",
        "    @staticmethod\n",
        "    def camera_center_in_board(rvec: np.ndarray, tvec: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the camera center in the chessboard/world coordinate system.\n",
        "\n",
        "        Uses the standard pinhole model:\n",
        "            X_c = R * X_w + t\n",
        "        → invert to get camera center in world coords:\n",
        "            C_w = -R^T * t\n",
        "\n",
        "        Args:\n",
        "            rvec: Rotation vector (Rodrigues, 3x1).\n",
        "            tvec: Translation vector (3x1).\n",
        "\n",
        "        Returns:\n",
        "            Camera center in board/world frame as a flat NumPy array (3,).\n",
        "        \"\"\"\n",
        "        R = Cam.rodrigues_to_R(rvec)\n",
        "        C = -R.T @ tvec.reshape(3, 1)\n",
        "        return C.flatten()\n"
      ],
      "metadata": {
        "id": "wztnn8350S48"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1) Data and Calibration\n",
        "\n"
      ],
      "metadata": {
        "id": "_cIm1l6Ni42v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Download demo images"
      ],
      "metadata": {
        "id": "hEf6V6uEgjGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Penguini128/computer-vision-demo-images"
      ],
      "metadata": {
        "id": "etvfda2RgjVy",
        "outputId": "34aaa126-a57a-4f6e-da8d-de10c4b872ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'computer-vision-demo-images' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-generate and save intrinsic parameters for demo locally"
      ],
      "metadata": {
        "id": "pkvTxREDg1kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKERBOARD = (9, 6)   # number of inner corners per a chessboard row and column\n",
        "SQUARE_SIZE = 25.4*6/7       # set to real size of a square if you want results in real units\n",
        "IMAGE_DIR = \"computer-vision-demo-images/*.jpeg\"  # path to your images\n",
        "SAVE_FILE = \"camera_params.npz\"\n",
        "\n",
        "# Create 3D points for the checkerboard corners, e.g. (0,0,0), (1,0,0), ...\n",
        "objp = np.zeros((CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\n",
        "objp[:, :2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\n",
        "# Scale points by SQUARE_SIZE\n",
        "objp *= SQUARE_SIZE\n",
        "\n",
        "# Arrays to store object points and image points\n",
        "objpoints = []  # 3D points in real world space\n",
        "imgpoints = []  # 2D points in image plane\n",
        "\n",
        "images = glob.glob(IMAGE_DIR)\n",
        "\n",
        "for fname in images:\n",
        "    img = cv2.imread(fname)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find the chess board corners\n",
        "    ret, corners = cv2.findChessboardCorners(gray, CHECKERBOARD, None)\n",
        "\n",
        "    if ret:\n",
        "        objpoints.append(objp)\n",
        "        # refine corner locations\n",
        "        corners2 = cv2.cornerSubPix(\n",
        "            gray, corners, (11, 11), (-1, -1),\n",
        "            criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "        )\n",
        "        imgpoints.append(corners2)\n",
        "\n",
        "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
        "    objpoints, imgpoints, gray.shape[::-1], None, None\n",
        ")\n",
        "\n",
        "data = {\n",
        "    'K' : mtx.tolist(),\n",
        "    'distCoeffs' : dist.tolist()\n",
        "}\n",
        "\n",
        "print(data)\n",
        "\n",
        "\n",
        "with open('intrinsics.json', 'w', encoding='utf-8') as f:\n",
        "  json.dump(data, f, indent=4)\n"
      ],
      "metadata": {
        "id": "-pJE1d7Fg068",
        "outputId": "c2014683-19c5-4d7c-f6ae-d5010c99634c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'K': [[3139.0015479697936, 0.0, 1545.6609743626898], [0.0, 3432.1716395257067, 1956.7897029216788], [0.0, 0.0, 1.0]], 'distCoeffs': [[0.09632588463289136, 0.22821855282178258, -0.02150773740467271, 0.002057752891564199, -1.5481491836997736]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intrinsics + Model Points"
      ],
      "metadata": {
        "id": "Dlz-Vz3EPmIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_intrinsics(json_path):\n",
        "    \"\"\"\n",
        "    Expect JSON with keys:\n",
        "      K: [[fx,0,cx],[0,fy,cy],[0,0,1]]\n",
        "      distCoeffs: [k1,k2,p1,p2,k3] (length 4,5,8, or 12 acceptable)\n",
        "    \"\"\"\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    K = np.array(data[\"K\"], dtype=float)\n",
        "    dist = np.array(data.get(\"distCoeffs\", []), dtype=float).reshape(-1,1) if data.get(\"distCoeffs\") is not None else None\n",
        "    return K, dist\n",
        "\n",
        "def generate_checkerboard_points(cols, rows, square_size):\n",
        "    \"\"\"\n",
        "    Returns planar model points (Z=0) in meters in row-major order of INNER corners.\n",
        "    cols = number of inner corners along x, rows along y.\n",
        "    \"\"\"\n",
        "    xs, ys = np.meshgrid(np.arange(cols), np.arange(rows))\n",
        "    pts = np.stack([xs.ravel(), ys.ravel()], axis=1).astype(float) * float(square_size)\n",
        "    return pts  # shape: (N,2), Z=0 implied\n"
      ],
      "metadata": {
        "id": "Di8gwMiiPmcx"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pose from a homography (explicit derivation)"
      ],
      "metadata": {
        "id": "_hlEjGKPUDMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Utilities: normalized DLT for Homography ----------\n",
        "def _normalize_points_2d(pts):\n",
        "    \"\"\"\n",
        "    Hartley normalization: translate to zero mean and scale so that\n",
        "    the mean Euclidean distance from the origin is sqrt(2).\n",
        "    Returns (T, pts_norm), where T is the 3x3 similarity transform.\n",
        "    \"\"\"\n",
        "    pts = np.asarray(pts, float)\n",
        "    assert pts.shape[1] == 2\n",
        "    mean = pts.mean(axis=0)\n",
        "    pts_c = pts - mean\n",
        "    mean_dist = np.mean(np.sqrt(np.sum(pts_c**2, axis=1)))\n",
        "    s = np.sqrt(2) / mean_dist if mean_dist > 0 else 1.0\n",
        "\n",
        "    T = np.array([\n",
        "        [s, 0, -s*mean[0]],\n",
        "        [0, s, -s*mean[1]],\n",
        "        [0, 0, 1.0]\n",
        "    ], dtype=float)\n",
        "\n",
        "    pts_h = np.hstack([pts, np.ones((len(pts),1))])\n",
        "    pts_n = (T @ pts_h.T).T\n",
        "    return T, pts_n[:, :2]\n",
        "\n",
        "def _homography_dlt(model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Compute H (3x3) such that  [u v 1]^T ~ H [X Y 1]^T\n",
        "    using normalized DLT (no OpenCV).\n",
        "    \"\"\"\n",
        "    X = np.asarray(model_pts_2d, float)\n",
        "    x = np.asarray(image_pts_2d, float)\n",
        "    assert X.shape[0] >= 4 and X.shape == x.shape and X.shape[1] == 2\n",
        "\n",
        "    # Normalize both sets\n",
        "    T_x, x_n = _normalize_points_2d(x)\n",
        "    T_X, X_n = _normalize_points_2d(X)\n",
        "\n",
        "    # Build design matrix A (2 rows per correspondence)\n",
        "    N = X.shape[0]\n",
        "    A = []\n",
        "    for i in range(N):\n",
        "        X_i, Y_i = X_n[i]\n",
        "        u_i, v_i = x_n[i]\n",
        "        A.append([0, 0, 0, -X_i, -Y_i, -1, v_i*X_i, v_i*Y_i, v_i])\n",
        "        A.append([X_i, Y_i, 1, 0, 0, 0, -u_i*X_i, -u_i*Y_i, -u_i])\n",
        "    A = np.asarray(A, float)\n",
        "\n",
        "    # Solve Ah=0 via SVD (last singular vector)\n",
        "    U, S, Vt = np.linalg.svd(A)\n",
        "    h = Vt[-1, :]\n",
        "    Hn = h.reshape(3,3)\n",
        "\n",
        "    # Denormalize: x ~ T_x^{-1} * Hn * T_X * X\n",
        "    H = np.linalg.inv(T_x) @ Hn @ T_X\n",
        "    # Scale so that H[2,2] = 1 (optional; avoids huge scale)\n",
        "    if abs(H[2,2]) > 1e-12:\n",
        "        H = H / H[2,2]\n",
        "    return H\n",
        "\n",
        "# ---------- Pose from Homography (no OpenCV) ----------\n",
        "def pose_from_homography(K, model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Given intrinsics K and planar correspondences (X,Y,0) -> (u,v),\n",
        "    compute pose [R|t] from homography, WITHOUT using OpenCV.\n",
        "\n",
        "    Steps:\n",
        "      1) H from normalized DLT:  x ~ H X\n",
        "      2) B = K^{-1} H = [r1 r2 t] up to scale\n",
        "      3) lambda from ||b1|| and ||b2||; r1 = lam*b1, r2 = lam*b2, r3 = r1 x r2\n",
        "      4) Orthonormalize R via SVD; enforce det(R)=+1\n",
        "      5) t = lam*b3\n",
        "      6) Cheirality: flip sign so average depth of model points is positive\n",
        "    Returns: (R, t, H)\n",
        "    \"\"\"\n",
        "    K = np.asarray(K, float)\n",
        "    H = _homography_dlt(model_pts_2d, image_pts_2d)\n",
        "\n",
        "    # Remove intrinsics\n",
        "    Kinv = np.linalg.inv(K)\n",
        "    B = Kinv @ H\n",
        "\n",
        "    b1, b2, b3 = B[:,0], B[:,1], B[:,2]\n",
        "\n",
        "    # Scale lambda: use both columns for stability\n",
        "    n1 = np.linalg.norm(b1)\n",
        "    n2 = np.linalg.norm(b2)\n",
        "    lam = 2.0 / (n1 + n2) if (n1 + n2) > 1e-12 else 1.0\n",
        "\n",
        "    r1 = lam * b1\n",
        "    r2 = lam * b2\n",
        "    r3 = np.cross(r1, r2)\n",
        "\n",
        "    R_approx = np.stack([r1, r2, r3], axis=1)\n",
        "\n",
        "    # Orthonormalize (closest rotation) via SVD\n",
        "    U, _, Vt = np.linalg.svd(R_approx)\n",
        "    R = U @ Vt\n",
        "    if np.linalg.det(R) < 0:\n",
        "        # Proper rotation\n",
        "        U[:,-1] *= -1\n",
        "        R = U @ Vt\n",
        "\n",
        "    t = lam * b3.reshape(3,)\n",
        "\n",
        "    # Cheirality: ensure points lie in front (positive depth)\n",
        "    # Depth for a world point [X,Y,0,1]^T is: z = (R[2,:] @ [X,Y,0]) + t[2]\n",
        "    X = np.asarray(model_pts_2d, float)\n",
        "    depths = (R[2,0]*X[:,0] + R[2,1]*X[:,1] + t[2])\n",
        "    if np.mean(depths) < 0:\n",
        "        # Flip sign (projectively equivalent)\n",
        "        R[:,0:2] *= -1\n",
        "        t *= -1\n",
        "        # Re-orthonormalize after flip just in case\n",
        "        U, _, Vt = np.linalg.svd(R)\n",
        "        R = U @ Vt\n",
        "        if np.linalg.det(R) < 0:\n",
        "            U[:,-1] *= -1\n",
        "            R = U @ Vt\n",
        "\n",
        "    return R, t, H\n"
      ],
      "metadata": {
        "id": "tWilwsmEUhgn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenCV Pose (solvePnP)"
      ],
      "metadata": {
        "id": "MFByVTBlUpIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pose_from_pnp(K, dist, model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Uses solvePnP with planar points lifted to Z=0.\n",
        "    model_pts_2d: (N,2) -> becomes (N,3) with Z=0\n",
        "    image_pts_2d: (N,2)\n",
        "    \"\"\"\n",
        "    obj3d = np.hstack([np.asarray(model_pts_2d, float), np.zeros((len(model_pts_2d),1), float)])\n",
        "    img2d = np.asarray(image_pts_2d, float)\n",
        "\n",
        "    ok, rvec, tvec = cv2.solvePnP(obj3d, img2d, K, dist if dist is not None and len(dist)>0 else None, flags=cv2.SOLVEPNP_ITERATIVE)\n",
        "    if not ok:\n",
        "        raise ValueError(\"solvePnP failed\")\n",
        "\n",
        "    R, _ = cv2.Rodrigues(rvec)\n",
        "    # Ensure proper rotation\n",
        "    U, _, Vt = np.linalg.svd(R)\n",
        "    R = U @ Vt\n",
        "    if np.linalg.det(R) < 0:\n",
        "        U[:,-1] *= -1\n",
        "        R = U @ Vt\n",
        "\n",
        "    t = tvec.reshape(3,)\n",
        "    return R, t"
      ],
      "metadata": {
        "id": "oASs1cF8UpeQ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projection + Overlay"
      ],
      "metadata": {
        "id": "mZwr8hLTYy4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_points_world_to_image(pts_w3d, K, dist, R, t):\n",
        "    rvec, _ = cv2.Rodrigues(R)\n",
        "    tvec = t.reshape(3,1)\n",
        "    pts_img, _ = cv2.projectPoints(np.asarray(pts_w3d, float), rvec, tvec, K, dist if dist is not None and len(dist)>0 else None)\n",
        "    return pts_img.reshape(-1,2)\n",
        "\n",
        "def overlay_axes(image_bgr, K, dist, R, t, axis_len=0.05):\n",
        "    \"\"\"\n",
        "    Draws XYZ axes at world origin (0,0,0) with length axis_len (meters).\n",
        "    \"\"\"\n",
        "    img = image_bgr.copy()\n",
        "    origin = np.array([[0,0,0]], dtype=float)\n",
        "    axes = np.array([[axis_len,0,0],\n",
        "                     [0,axis_len,0],\n",
        "                     [0,0,axis_len]], dtype=float)\n",
        "\n",
        "    origin_px = project_points_world_to_image(origin, K, dist, R, t)[0]\n",
        "    axes_px = project_points_world_to_image(axes, K, dist, R, t)\n",
        "\n",
        "    O = tuple(np.round(origin_px).astype(int))\n",
        "    X = tuple(np.round(axes_px[0]).astype(int))\n",
        "    Y = tuple(np.round(axes_px[1]).astype(int))\n",
        "    Z = tuple(np.round(axes_px[2]).astype(int))\n",
        "\n",
        "    cv2.line(img, O, X, (255,0,0), 2)\n",
        "    cv2.line(img, O, Y, (0,255,0), 2)\n",
        "    cv2.line(img, O, Z, (0,0,255), 2)\n",
        "    return img"
      ],
      "metadata": {
        "id": "EOPL8O08YzL3"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D Camera Plot"
      ],
      "metadata": {
        "id": "0A8EfEsgZCrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_camera_poses_3d(\n",
        "    pose_list: List[Tuple[str, np.ndarray, np.ndarray]],\n",
        "    square_size_m: float\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize estimated camera centers in the chessboard/world frame.\n",
        "\n",
        "    Args:\n",
        "        pose_list: List of (image_path, rvec, tvec) for each image with a solved pose.\n",
        "                   rvec/tvec express the camera pose w.r.t. the board (world) frame.\n",
        "        square_size_m: Real-world size of one chessboard square (for axis/grid scaling).\n",
        "\n",
        "    Returns:\n",
        "        Plotly Figure with:\n",
        "          - A faint board grid lying on Z=0 (world XY plane)\n",
        "          - Camera centers as markers labeled #0, #1, …\n",
        "          - Board axes (X, Y, Z) drawn at the origin\n",
        "\n",
        "    Notes:\n",
        "        - World frame convention (from earlier):\n",
        "            Board lies in Z=0; units are meters if `square_size_m` is in meters.\n",
        "        - Camera center is computed as C_w = -R^T * t (already done upstream).\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "    if len(pose_list) == 0:\n",
        "        # Nothing to plot → keep consistent aspect mode so axes don't distort\n",
        "        fig.update_layout(title=\"No poses to display\", scene_aspectmode='data')\n",
        "        return fig\n",
        "\n",
        "\n",
        "    # Camera centers\n",
        "    centers, labels = [], []\n",
        "    for pth, rvec, tvec in pose_list:\n",
        "        C = Cam.camera_center_in_board(rvec, tvec)   # shape (3,)\n",
        "        centers.append(C)                            # keep as array/list; we'll cast below\n",
        "        labels.append(os.path.basename(pth))\n",
        "\n",
        "    # Cast to NumPy BEFORE any [:, ...] indexing\n",
        "    centers = np.asarray(centers, dtype=float)       # shape (N, 3)\n",
        "\n",
        "    # Flip Z so cameras in front of the board appear at +Z (matches overlay)\n",
        "    centers[:, 2] *= -1\n",
        "\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=centers[:, 0], y=centers[:, 1], z=centers[:, 2],\n",
        "        mode=\"markers+text\",\n",
        "        text=[f\"#{i}\" for i in range(len(labels))],\n",
        "        textposition=\"top center\",\n",
        "        marker=dict(size=4), name=\"Camera centers\"\n",
        "    ))\n",
        "\n",
        "    # --- Board axes drawn at the origin (0,0,0) ---\n",
        "    L = 3.0 * square_size_m  # axis length in world units\n",
        "    axes = {\n",
        "        \"X\": [[0, L], [0, 0], [0, 0]],\n",
        "        \"Y\": [[0, 0], [0, L], [0, 0]],\n",
        "        \"Z\": [[0, 0], [0, 0], [0, L]],\n",
        "    }\n",
        "    for name, (ax, ay, az) in axes.items():\n",
        "        fig.add_trace(go.Scatter3d(x=ax, y=ay, z=az, mode=\"lines\", name=f\"{name}-axis\"))\n",
        "\n",
        "    # Keep aspect ratio true to data scale; label axes with units\n",
        "    fig.update_layout(\n",
        "        title=\"Estimated Camera Poses w.r.t. Chessboard\",\n",
        "        scene=dict(\n",
        "            xaxis_title=\"X (m)\", yaxis_title=\"Y (m)\", zaxis_title=\"Z (m)\",\n",
        "            aspectmode=\"data\"\n",
        "        ),\n",
        "        legend=dict(x=0, y=1.0)\n",
        "    )\n",
        "    return fig"
      ],
      "metadata": {
        "id": "ljLi7Up-ZDR9"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Bf7srHmeGbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ROBUST CHESSBOARD AUTO-DETECT (drop-in replacement for on_auto_detect + helpers) ---\n",
        "import cv2, numpy as np\n",
        "\n",
        "def _maybe_undistort(bgr, K, dist):\n",
        "    if K is None or dist is None or len(dist) == 0:\n",
        "        return bgr, K\n",
        "    h, w = bgr.shape[:2]\n",
        "    newK, _ = cv2.getOptimalNewCameraMatrix(K, dist, (w, h), 0)  # alpha=0 (crop)\n",
        "    und = cv2.undistort(bgr, K, dist, None, newK)\n",
        "    return und, newK\n",
        "\n",
        "def _prep_variants(bgr):\n",
        "    \"\"\"Generate a few variants to help detection.\"\"\"\n",
        "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
        "    v = []\n",
        "\n",
        "    # base\n",
        "    v.append(gray)\n",
        "\n",
        "    # equalized\n",
        "    v.append(cv2.equalizeHist(gray))\n",
        "\n",
        "    # mild blur\n",
        "    v.append(cv2.GaussianBlur(gray, (3,3), 0))\n",
        "\n",
        "    # slightly downscaled (sometimes helps SB)\n",
        "    small = cv2.resize(gray, None, fx=0.7, fy=0.7, interpolation=cv2.INTER_AREA)\n",
        "    v.append(small)\n",
        "\n",
        "    # inverted (white/black contrast reversal sometimes helps)\n",
        "    v.append(255 - gray)\n",
        "\n",
        "    return v\n",
        "\n",
        "def _try_find(gray, pattern_size):\n",
        "    flags = cv2.CALIB_CB_ADAPTIVE_THRESH | cv2.CALIB_CB_NORMALIZE_IMAGE\n",
        "    ok, corners = cv2.findChessboardCorners(gray, pattern_size, flags=flags)\n",
        "    if ok and corners is not None:\n",
        "        # refine on original scale only\n",
        "        if gray.ndim == 2 and gray.shape[0] >= 50 and gray.shape[1] >= 50:\n",
        "            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 60, 1e-6)\n",
        "            cv2.cornerSubPix(gray, corners, (5,5), (-1,-1), criteria)\n",
        "        return True, corners.reshape(-1,2).astype(np.float32)\n",
        "\n",
        "    # SB fallback (often more robust)\n",
        "    ok, corners = cv2.findChessboardCornersSB(gray, pattern_size, flags=0)\n",
        "    if ok and corners is not None:\n",
        "        return True, corners.reshape(-1,2).astype(np.float32)\n",
        "\n",
        "    return False, None\n",
        "\n",
        "def robust_auto_detect(bgr, cols, rows, K=None, dist=None, detect_on_undistorted=False):\n",
        "    \"\"\"\n",
        "    Tries: raw/undistorted, multiple image variants, and a couple of nearby pattern sizes.\n",
        "    Returns (ok, corners, used_cols, used_rows, used_K)\n",
        "    \"\"\"\n",
        "    # Candidate inner-corner sizes (try requested first, then neighbors)\n",
        "    sizes = [(int(cols), int(rows))]\n",
        "    # Add a couple of nearby guesses (common off-by-one)\n",
        "    neighbor_sizes = {(int(cols)-1, int(rows)),\n",
        "                      (int(cols), int(rows)-1),\n",
        "                      (int(cols)+1, int(rows)),\n",
        "                      (int(cols), int(rows)+1)}\n",
        "    sizes += [s for s in neighbor_sizes if s[0] > 2 and s[1] > 2]\n",
        "\n",
        "    # Choose raw vs undistorted for detection\n",
        "    if detect_on_undistorted:\n",
        "        work_bgr, work_K = _maybe_undistort(bgr, K, dist)\n",
        "    else:\n",
        "        work_bgr, work_K = bgr, K\n",
        "\n",
        "    variants = _prep_variants(work_bgr)\n",
        "    for (c,r) in sizes:\n",
        "        pat = (c, r)\n",
        "        for g in variants:\n",
        "            ok, corners = _try_find(g, pat)\n",
        "            if ok:\n",
        "                # If we detected on a resized 'small', rescale back up\n",
        "                if g.shape[:2] != cv2.cvtColor(work_bgr, cv2.COLOR_BGR2GRAY).shape[:2]:\n",
        "                    sy = work_bgr.shape[0] / g.shape[0]\n",
        "                    sx = work_bgr.shape[1] / g.shape[1]\n",
        "                    corners = corners * np.array([sx, sy], dtype=np.float32)\n",
        "                return True, corners, c, r, work_K\n",
        "    return False, None, int(cols), int(rows), work_K\n",
        "\n",
        "def on_auto_detect(cols, rows, detect_on_undistorted=True):\n",
        "    \"\"\"\n",
        "    Gradio callback: fills CLICK_STATE['clicks'] with detected corners in row-major order.\n",
        "    \"\"\"\n",
        "    if CLICK_STATE[\"img_bgr\"] is None:\n",
        "        return None, \"No image uploaded.\"\n",
        "\n",
        "    bgr = CLICK_STATE[\"img_bgr\"]\n",
        "    K = CLICK_STATE[\"K\"]\n",
        "    dist = CLICK_STATE[\"dist\"]\n",
        "\n",
        "    ok, corners, used_c, used_r, usedK = robust_auto_detect(\n",
        "        bgr, cols, rows, K=K, dist=dist, detect_on_undistorted=bool(detect_on_undistorted)\n",
        "    )\n",
        "    if not ok:\n",
        "        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), f\"Auto-detect failed after robust tries (asked {cols}×{rows}). Tip: verify inner-corner counts.\"\n",
        "\n",
        "    # Push into click list\n",
        "    CLICK_STATE[\"clicks\"] = [(float(x), float(y)) for (x,y) in corners]\n",
        "\n",
        "    # Visualize sparsely to avoid clutter\n",
        "    vis = bgr.copy()\n",
        "    for i,(x,y) in enumerate(CLICK_STATE[\"clicks\"]):\n",
        "        cv2.circle(vis, (int(x), int(y)), 4, (0,255,255), -1)\n",
        "        if i % (used_c//2 if used_c>6 else 4) == 0:\n",
        "            cv2.putText(vis, str(i), (int(x)+4, int(y)-4), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0,255,255), 1)\n",
        "\n",
        "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB), f\"Clicks: {len(CLICK_STATE['clicks'])} (detected {used_c}×{used_r})\"\n"
      ],
      "metadata": {
        "id": "lSAuUS4GeG4z"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pose Helpers"
      ],
      "metadata": {
        "id": "U_wvayxNvm_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Unified estimation callback: computes (R,t) from Homography and OpenCV, overlay, and 3D plot ===\n",
        "import io, json\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.figure import Figure\n",
        "\n",
        "def _pretty_matrix(M):\n",
        "    if M is None:\n",
        "        return \"None\"\n",
        "    return np.array2string(np.asarray(M), precision=5, suppress_small=True)\n",
        "\n",
        "def _normalize_corners(c):\n",
        "    \"\"\"\n",
        "    Accepts many shapes/types (tuple, list, Nx1x2, Nx2) and returns Nx2 float64 or None.\n",
        "    Also pulls the first array-like from tuples like (corners, meta).\n",
        "    \"\"\"\n",
        "    if c is None:\n",
        "        return None\n",
        "    if isinstance(c, tuple):\n",
        "        for item in c:\n",
        "            if hasattr(item, \"shape\") or isinstance(item, (list, tuple)):\n",
        "                c = item\n",
        "                break\n",
        "    c = np.array(c, dtype=np.float64)\n",
        "    if c.ndim == 3 and c.shape[1] == 1 and c.shape[2] == 2:\n",
        "        c = c.reshape(-1, 2)\n",
        "    if c.ndim == 2 and c.shape[1] == 2:\n",
        "        return c\n",
        "    return None\n",
        "\n",
        "def on_estimate_pose(image_bgr, intr_file, rows, cols, square_size_m, use_undistort, use_solvepnp_ransac=True):\n",
        "    # 0) Validate inputs\n",
        "    if image_bgr is None:\n",
        "        return \"No image\", \"\", \"No image\", \"\", None, None\n",
        "    if intr_file is None:\n",
        "        return \"No intrinsics\", \"\", \"No intrinsics\", \"\", None, None\n",
        "\n",
        "    # 1) Load intrinsics (support File object or string path)\n",
        "    try:\n",
        "        intr_path = getattr(intr_file, \"name\", None) or (intr_file if isinstance(intr_file, str) else None)\n",
        "        with open(intr_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        K = np.array(data.get(\"K\") or data.get(\"camera_matrix\"), dtype=float)\n",
        "        D_key = None\n",
        "        for k in [\"D\", \"distortion_coefficients\", \"distCoeffs\", \"dist\"]:\n",
        "            if k in data:\n",
        "                D_key = k\n",
        "                break\n",
        "        dist = np.array(data[D_key], dtype=float).reshape(-1,1) if D_key else np.zeros((5,1),dtype=float)\n",
        "    except Exception as e:\n",
        "        return f\"Bad intrinsics: {e}\", \"\", f\"Bad intrinsics: {e}\", \"\", None, None\n",
        "\n",
        "    # 2) Prepare image (undistort if requested)\n",
        "    bgr = image_bgr.copy()\n",
        "    if use_undistort:\n",
        "        h, w = bgr.shape[:2]\n",
        "        newK, _ = cv.getOptimalNewCameraMatrix(K, dist, (w,h), 0)\n",
        "        bgr = cv.undistort(bgr, K, dist, None, newK)\n",
        "        use_K = newK\n",
        "        use_dist = np.zeros((dist.size,1), dtype=float)\n",
        "    else:\n",
        "        use_K = K\n",
        "        use_dist = dist\n",
        "\n",
        "    # 3) Detect corners\n",
        "    corners = None\n",
        "    try:\n",
        "        corners = robust_auto_detect(bgr, int(rows), int(cols))\n",
        "    except Exception:\n",
        "        pass\n",
        "    if corners is None:\n",
        "        try:\n",
        "            gray = cv.cvtColor(bgr, cv.COLOR_BGR2GRAY)\n",
        "            ok, pts = cv.findChessboardCorners(gray, (int(cols), int(rows)), None)\n",
        "            if ok:\n",
        "                pts = cv.cornerSubPix(gray, pts, (11,11), (-1,-1),\n",
        "                                      criteria=(cv.TERM_CRITERIA_EPS+cv.TERM_CRITERIA_MAX_ITER, 30, 0.001))\n",
        "                corners = pts\n",
        "        except Exception:\n",
        "            corners = None\n",
        "\n",
        "    corners = _normalize_corners(corners)\n",
        "    if corners is None or len(corners) < 4:\n",
        "        return \"No usable corners found\", \"\", \"No usable corners found\", \"\", None, None\n",
        "\n",
        "    # 4) Build world points\n",
        "    objp = generate_checkerboard_points(int(rows), int(cols), float(square_size_m))  # (N,3)\n",
        "\n",
        "    # 5a) Homography -> Pose (your API expects K, model_pts_2d, image_pts_2d)\n",
        "    try:\n",
        "        img_pts  = corners.astype(np.float64)           # shape (N,2)\n",
        "        wrld_pts = objp[:, :2].astype(np.float64)       # (X,Y) only\n",
        "        R_H, t_H, H = pose_from_homography(use_K, wrld_pts, img_pts)\n",
        "        h_R_txt = _pretty_matrix(R_H)\n",
        "        h_t_txt = _pretty_matrix(t_H.reshape(3,1))\n",
        "    except Exception as e:\n",
        "        h_R_txt = f\"Homography failed: {e}\"\n",
        "        h_t_txt = \"\"\n",
        "\n",
        "\n",
        "    # 5b) OpenCV PnP -> Pose (robust to type/shape/count)\n",
        "    cv_R_txt, cv_t_txt = \"solvePnP failed\", \"\"\n",
        "    R_cv, tvec = None, None\n",
        "    try:\n",
        "        # Ensure float32 and contiguous for OpenCV\n",
        "        obj_pts = np.ascontiguousarray(objp.astype(np.float32).reshape(-1,1,3))   # (N,1,3)\n",
        "        img_pts = np.ascontiguousarray(corners.astype(np.float32).reshape(-1,1,2))# (N,1,2)\n",
        "\n",
        "        # Enforce equal count and at least 4 points\n",
        "        N = min(len(obj_pts), len(img_pts))\n",
        "        obj_pts, img_pts = obj_pts[:N], img_pts[:N]\n",
        "        if N >= 4:\n",
        "            cv_success, rvec, tvec = cv.solvePnP(\n",
        "                obj_pts, img_pts,\n",
        "                use_K.astype(np.float64),\n",
        "                use_dist.astype(np.float64),\n",
        "                flags=cv.SOLVEPNP_ITERATIVE\n",
        "            )\n",
        "            if cv_success:\n",
        "                R_cv, _ = cv.Rodrigues(rvec)\n",
        "                cv_R_txt = _pretty_matrix(R_cv)\n",
        "                cv_t_txt = _pretty_matrix(tvec.reshape(3,1))\n",
        "        else:\n",
        "            cv_R_txt = f\"solvePnP needs ≥ 4 points; only got {N}\"\n",
        "            cv_t_txt = \"\"\n",
        "    except Exception as e:\n",
        "        cv_R_txt = f\"OpenCV failed: {e}\"\n",
        "        cv_t_txt = \"\"\n",
        "\n",
        "\n",
        "\n",
        "    # 6) Overlay reprojection sanity check\n",
        "    try:\n",
        "        use_R = R_cv if R_cv is not None else (R_H if 'R_H' in locals() else None)\n",
        "        use_t = tvec.reshape(3,1) if tvec is not None else (t_H.reshape(3,1) if 't_H' in locals() else None)\n",
        "        overlay = bgr.copy()\n",
        "        if use_R is not None and use_t is not None:\n",
        "            overlay = overlay_axes(overlay, use_K, use_dist, use_R, use_t, axis_len=float(square_size_m)*3.0)\n",
        "        overlay_rgb = cv.cvtColor(overlay, cv.COLOR_BGR2RGB)\n",
        "    except Exception:\n",
        "        overlay_rgb = None\n",
        "\n",
        "    # 7) 3D plot — your plotly-based function expects: [(label, rvec, tvec)], square_size_m\n",
        "    try:\n",
        "        entries = []\n",
        "\n",
        "        # OpenCV pose (if available)\n",
        "        if R_cv is not None and tvec is not None:\n",
        "            # If rvec exists from solvePnP, use it; otherwise derive from R_cv\n",
        "            rvec_use = rvec if 'rvec' in locals() and rvec is not None else cv.Rodrigues(R_cv)[0]\n",
        "            entries.append((\n",
        "                \"OpenCV\",\n",
        "                np.asarray(rvec_use, dtype=np.float64).reshape(3,1),\n",
        "                np.asarray(tvec, dtype=np.float64).reshape(3,1),\n",
        "            ))\n",
        "\n",
        "        # Homography pose (if available)\n",
        "        if 'R_H' in locals() and R_H is not None and 't_H' in locals() and t_H is not None:\n",
        "            rvec_H = cv.Rodrigues(R_H)[0]\n",
        "            entries.append((\n",
        "                \"Homography\",\n",
        "                np.asarray(rvec_H, dtype=np.float64).reshape(3,1),\n",
        "                np.asarray(t_H, dtype=np.float64).reshape(3,1),\n",
        "            ))\n",
        "\n",
        "        if entries:\n",
        "            fig = plot_camera_poses_3d(entries, float(square_size_m))\n",
        "        else:\n",
        "            # Visible placeholder so the panel never looks empty\n",
        "            fig = go.Figure()\n",
        "            fig.update_layout(title=\"No pose available (Homography & OpenCV both missing)\", height=320)\n",
        "    except Exception as e:\n",
        "        # Show the error in the plot panel instead of silently returning None\n",
        "        fig = go.Figure()\n",
        "        fig.add_annotation(text=f\"3D plot error: {e}\", x=0.5, y=0.5, showarrow=False)\n",
        "        fig.update_layout(title=\"3D Camera Pose (error)\", height=320)\n",
        "\n",
        "\n",
        "    return h_R_txt, h_t_txt, cv_R_txt, cv_t_txt, overlay_rgb, fig\n"
      ],
      "metadata": {
        "id": "J-aSN6K2vnom"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimal Gradio UI (ordered clicks + undo)"
      ],
      "metadata": {
        "id": "BcNKBvNFZMSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --- State ---\n",
        "CLICK_STATE = {\n",
        "    \"img_bgr\": None,\n",
        "    \"display_img_bgr\": None,\n",
        "    \"clicks\": [],\n",
        "    \"K\": None,\n",
        "    \"dist\": None,\n",
        "    \"model_pts_2d\": None\n",
        "}\n",
        "\n",
        "# --- Load Image ---\n",
        "def on_image_upload(img):\n",
        "    if img is None:\n",
        "        return None, \"No image\"\n",
        "    bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    CLICK_STATE[\"img_bgr\"] = bgr\n",
        "\n",
        "    (h, w) = bgr.shape[:2]\n",
        "    desired_height = 800\n",
        "    ratio = desired_height / float(h)\n",
        "    new_width_aspect = int(w * ratio)\n",
        "    resized_image = cv2.resize(bgr, (new_width_aspect, desired_height))\n",
        "\n",
        "    CLICK_STATE[\"display_img_bgr\"] = resized_image\n",
        "    CLICK_STATE[\"clicks\"].clear()\n",
        "    return cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB), \"Image loaded\"\n",
        "\n",
        "# --- Load Intrinsics ---\n",
        "def on_intrinsics_upload(file_obj):\n",
        "    if file_obj is None:\n",
        "        return \"No intrinsics file.\"\n",
        "    K, dist = load_intrinsics(file_obj.name)\n",
        "    CLICK_STATE[\"K\"] = K\n",
        "    CLICK_STATE[\"distCoeffs\"] = dist\n",
        "    return f\"Loaded K and distCoeffs...\\n\\nK = \\n{K}\\n\\ndistCoeffs = {dist}\"\n",
        "\n",
        "def draw_outlined_text(img, text, pos, font, font_size, font_color, text_size, outline_thickness):\n",
        "    outline_color = (255, 255, 255)\n",
        "    if sum(font_color) >= 255:\n",
        "        ouline_color = (0, 0, 0)\n",
        "    cv2.putText(img, text, pos, font, font_size, ouline_color, text_size + outline_thickness)\n",
        "    cv2.putText(img, text, pos, font, font_size, font_color, text_size)\n",
        "\n",
        "\n",
        "# --- Manual Click Handler ---\n",
        "def on_click(evt: gr.SelectData):\n",
        "    CLICK_STATE[\"clicks\"].append((evt.index[0], evt.index[1]))\n",
        "    vis = CLICK_STATE[\"display_img_bgr\"].copy()\n",
        "    for i, (x,y) in enumerate(CLICK_STATE[\"clicks\"]):\n",
        "        cv2.circle(vis, (int(x),int(y)), 10, (0,255,0), -1)\n",
        "        draw_outlined_text(vis, str(i+1), (int(x)+5,int(y)-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 3, 2)\n",
        "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB), f\"Clicks: {len(CLICK_STATE['clicks'])}\"\n",
        "\n",
        "# --- Auto-detect (stub: replace with your robust_auto_detect) ---\n",
        "def auto_detect_corners(cols, rows):\n",
        "    bgr = CLICK_STATE[\"img_bgr\"]\n",
        "    if bgr is None:\n",
        "        return None, \"No image loaded\"\n",
        "    # Fake detection for now: top-left 4 points\n",
        "    h, w = bgr.shape[:2]\n",
        "    corners = [(50,50),(150,50),(250,50),(350,50)]\n",
        "    CLICK_STATE[\"clicks\"] = corners\n",
        "    vis = bgr.copy()\n",
        "    for i,(x,y) in enumerate(corners):\n",
        "        cv2.circle(vis, (int(x),int(y)), 4, (0,255,0), -1)\n",
        "        draw_outlined_text(vis, str(i+1), (int(x)+5,int(y)-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 3, 2)\n",
        "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB), f\"Auto: {len(corners)} points\"\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Pose from a Planar Object — Manual or Auto Points\")\n",
        "\n",
        "    with gr.Row():\n",
        "        img = gr.Image(type=\"numpy\", label=\"Upload planar scene\")\n",
        "        img_status = gr.Textbox(label=\"Image Status\", value=\"Awaiting image upload...\", interactive=False)\n",
        "        intr = gr.File(label=\"Upload intrinsics JSON\")\n",
        "        status_intr = gr.Textbox(label=\"Intrinsics status\",value=\"Awaiting JSON upload...\", interactive=False)\n",
        "        img.upload(on_image_upload, img, [img, img_status])\n",
        "        img.clear(lambda: \"Awaiting image upload...\", outputs=img_status)\n",
        "        intr.upload(on_intrinsics_upload, intr, status_intr)\n",
        "        intr.clear(lambda: \"Awaiting JSON upload...\", outputs=status_intr)\n",
        "\n",
        "    with gr.Row():\n",
        "        auto = gr.Button(\"Auto-detect Corners\")\n",
        "        clear = gr.Button(\"Clear Clicks\")\n",
        "\n",
        "    click_info = gr.Textbox(label=\"Click info\")\n",
        "\n",
        "    # manual clicks happen only here\n",
        "    img.select(on_click, None, [img, click_info])\n",
        "    auto.click(auto_detect_corners, [gr.Number(9), gr.Number(6)], [img, click_info])\n",
        "    clear.click(lambda: (CLICK_STATE[\"clicks\"].clear(), \"Clicks cleared\")[1], None, click_info)\n",
        "\n",
        "    # --- Pose estimation controls & outputs ---\n",
        "    with gr.Row():\n",
        "        rows_in = gr.Number(label=\"Rows (inner corners)\", value=6, precision=0)\n",
        "        cols_in = gr.Number(label=\"Cols (inner corners)\", value=9, precision=0)\n",
        "        sq_in = gr.Number(label=\"Square size (meters)\", value=25.4*6/7, precision=6)\n",
        "        use_undistort = gr.Checkbox(label=\"Undistort image before estimation\", value=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        btn_estimate = gr.Button(\"Estimate Pose\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        h_R = gr.Textbox(label=\"Homography → Pose: R\", lines=6, interactive=False)\n",
        "        h_t = gr.Textbox(label=\"Homography → Pose: t\", lines=3, interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        cv_R = gr.Textbox(label=\"OpenCV (solvePnP): R\", lines=6, interactive=False)\n",
        "        cv_t = gr.Textbox(label=\"OpenCV (solvePnP): t\", lines=3, interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        overlay_img = gr.Image(label=\"Overlay: reprojected axes / points\", type=\"numpy\")\n",
        "        pose_fig = gr.Plot(label=\"3D Camera Pose (world frame)\")\n",
        "\n",
        "    # Wire up the click: uses shared inputs img and intr from above\n",
        "    btn_estimate.click(\n",
        "        fn=on_estimate_pose,\n",
        "        inputs=[img, intr, rows_in, cols_in, sq_in, use_undistort],\n",
        "        outputs=[h_R, h_t, cv_R, cv_t, overlay_img, pose_fig]\n",
        "    )"
      ],
      "metadata": {
        "id": "ckC6nCsMZM2i"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(inline=True)"
      ],
      "metadata": {
        "id": "UDPBlupBkOXg",
        "outputId": "31954120-6b06-4730-fc98-39636a7e5456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eeca7002d74e585663.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eeca7002d74e585663.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Notes (Homography→Pose vs OpenCV)\n",
        "TODO FINISH THESE!!!!"
      ],
      "metadata": {
        "id": "qj9bqrMXbIBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- R matrices were (similar / slightly different); translation differs up to a common **scale** in homography path.\n",
        "- With undistorted image vs passing `distCoeffs`, results (were / were not) consistent; I chose (approach) for both paths.\n",
        "- With >4 points and mild noise, solvePnP was more stable; homography was sensitive to point order & spread across plane.\n",
        "- Enforcing `det(R)=+1` and SVD re-orthonormalization removed small drift from numeric noise.\n",
        "- Cheirality: not an issue with `solvePnP`; with `decomposeHomographyMat`, select solution with positive depths.\n",
        "- RANSAC for H improves robustness to a bad click but may drop to minimal 4 inliers → less stable pose."
      ],
      "metadata": {
        "id": "YtN8R4rMbP5W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIuA0ynrkV_X"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggmeiner22/Estimating-Camera-Pose-from-a-Planar-Object/blob/main/EstimatingCameraPoseFromAPlanarObject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal\n",
        "\n",
        "Given a calibrated camera (i.e., known intrinsics **K**) and a single image of a planar object, estimate the camera **pose** (i.e., extrinsics **R**, **t**).\n",
        "\n",
        "I will:\n",
        "\n",
        "1. build a Gradio UI to click 2D features and manage point order,\n",
        "2. estimate pose **from a homography** (explicit derivation), and\n",
        "3. estimate pose using **OpenCV** functions,\n",
        "4. compare the two (short notes only)."
      ],
      "metadata": {
        "id": "EpbqkAzHIM5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs dependencies"
      ],
      "metadata": {
        "id": "CWn5jpoaI94w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B2u8_q2_IJ7l"
      },
      "outputs": [],
      "source": [
        "%pip -q install opencv-python numpy gradio matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "SqNF6-vkLfzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, io, math, os\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "from matplotlib.figure import Figure\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import plotly.graph_objects as go         # Interactive plotting (3D/2D)"
      ],
      "metadata": {
        "id": "yyqTxE69LgHW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Camera Class"
      ],
      "metadata": {
        "id": "oapejCBh0SXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cam:\n",
        "    \"\"\"\n",
        "    Camera-related utilities for calibration and pose handling.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def rodrigues_to_R(rvec: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Convert a Rodrigues rotation vector to a 3x3 rotation matrix.\n",
        "\n",
        "        Args:\n",
        "            rvec: Rotation vector (3x1 or 1x3), Rodrigues representation.\n",
        "\n",
        "        Returns:\n",
        "            R: Rotation matrix (3x3).\n",
        "        \"\"\"\n",
        "        R, _ = cv.Rodrigues(rvec)\n",
        "        return R\n",
        "\n",
        "    @staticmethod\n",
        "    def camera_center_in_board(rvec: np.ndarray, tvec: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the camera center in the chessboard/world coordinate system.\n",
        "\n",
        "        Uses the standard pinhole model:\n",
        "            X_c = R * X_w + t\n",
        "        â†’ invert to get camera center in world coords:\n",
        "            C_w = -R^T * t\n",
        "\n",
        "        Args:\n",
        "            rvec: Rotation vector (Rodrigues, 3x1).\n",
        "            tvec: Translation vector (3x1).\n",
        "\n",
        "        Returns:\n",
        "            Camera center in board/world frame as a flat NumPy array (3,).\n",
        "        \"\"\"\n",
        "        R = Cam.rodrigues_to_R(rvec)\n",
        "        C = -R.T @ tvec.reshape(3, 1)\n",
        "        return C.flatten()\n"
      ],
      "metadata": {
        "id": "wztnn8350S48"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1) Data and Calibration\n",
        "\n"
      ],
      "metadata": {
        "id": "_cIm1l6Ni42v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Download demo images"
      ],
      "metadata": {
        "id": "hEf6V6uEgjGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Penguini128/computer-vision-demo-images"
      ],
      "metadata": {
        "id": "etvfda2RgjVy",
        "outputId": "3f493f70-fb15-4b1f-af88-ad5fd82c5190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'computer-vision-demo-images' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-generate and save intrinsic parameters for demo locally"
      ],
      "metadata": {
        "id": "pkvTxREDg1kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKERBOARD = (9, 6)   # number of inner corners per a chessboard row and column\n",
        "SQUARE_SIZE = 25.4*6/7       # set to real size of a square if you want results in real units\n",
        "IMAGE_DIR = \"computer-vision-demo-images/*.jpeg\"  # path to your images\n",
        "SAVE_FILE = \"camera_params.npz\"\n",
        "\n",
        "# Create 3D points for the checkerboard corners, e.g. (0,0,0), (1,0,0), ...\n",
        "objp = np.zeros((CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\n",
        "objp[:, :2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\n",
        "# Scale points by SQUARE_SIZE\n",
        "objp *= SQUARE_SIZE\n",
        "\n",
        "# Arrays to store object points and image points\n",
        "objpoints = []  # 3D points in real world space\n",
        "imgpoints = []  # 2D points in image plane\n",
        "\n",
        "images = glob.glob(IMAGE_DIR)\n",
        "\n",
        "images_drawChessboard = []\n",
        "images_manualOverlay = []\n",
        "\n",
        "for fname in images:\n",
        "    img = cv.imread(fname)\n",
        "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find the chess board corners\n",
        "    ret, corners = cv.findChessboardCorners(gray, CHECKERBOARD, None)\n",
        "\n",
        "    if ret:\n",
        "        objpoints.append(objp)\n",
        "        # refine corner locations\n",
        "        corners2 = cv.cornerSubPix(\n",
        "            gray, corners, (11, 11), (-1, -1),\n",
        "            criteria=(cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "        )\n",
        "        imgpoints.append(corners2)\n",
        "\n",
        "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(\n",
        "    objpoints, imgpoints, gray.shape[::-1], None, None\n",
        ")\n",
        "\n",
        "data = {\n",
        "    'K' : mtx.tolist(),\n",
        "    'distCoeffs' : dist.tolist()\n",
        "}\n",
        "\n",
        "print(data)\n",
        "\n",
        "\n",
        "with open('intrinsics.json', 'w', encoding='utf-8') as f:\n",
        "  json.dump(data, f, indent=4)\n"
      ],
      "metadata": {
        "id": "-pJE1d7Fg068",
        "outputId": "88c55be9-1d47-410a-a116-93ac7cfd40af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'K': [[3139.00163360624, 0.0, 1545.6609755676743], [0.0, 3432.171618232842, 1956.7894838280108], [0.0, 0.0, 1.0]], 'distCoeffs': [[0.09632589221060667, 0.22821834740594504, -0.02150774498659025, 0.0020577530325651522, -1.5481473517760895]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intrinsics + Model Points"
      ],
      "metadata": {
        "id": "Dlz-Vz3EPmIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_intrinsics(json_path):\n",
        "    \"\"\"\n",
        "    Expect JSON with keys:\n",
        "      K: [[fx,0,cx],[0,fy,cy],[0,0,1]]\n",
        "      distCoeffs: [k1,k2,p1,p2,k3] (length 4,5,8, or 12 acceptable)\n",
        "    \"\"\"\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    K = np.array(data[\"K\"], dtype=float)\n",
        "    dist = np.array(data.get(\"distCoeffs\", []), dtype=float).reshape(-1,1) if data.get(\"distCoeffs\") is not None else None\n",
        "    return K, dist\n",
        "\n",
        "def generate_checkerboard_points(cols, rows, square_size):\n",
        "    \"\"\"\n",
        "    Returns planar model points (Z=0) in meters in row-major order of INNER corners.\n",
        "    cols = number of inner corners along x, rows along y.\n",
        "    \"\"\"\n",
        "    xs, ys = np.meshgrid(np.arange(cols), np.arange(rows))\n",
        "    pts = np.stack([xs.ravel(), ys.ravel()], axis=1).astype(float) * float(square_size)\n",
        "    return pts  # shape: (N,2), Z=0 implied\n"
      ],
      "metadata": {
        "id": "Di8gwMiiPmcx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pose from a homography (explicit derivation)"
      ],
      "metadata": {
        "id": "_hlEjGKPUDMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _homography_dlt(model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Compute H (3x3) such that  [u v 1]^T ~ H [X Y 1]^T\n",
        "    using normalized DLT (no OpenCV).\n",
        "    \"\"\"\n",
        "    world = np.asarray(model_pts_2d, float)\n",
        "    image = np.asarray(image_pts_2d, float)\n",
        "    assert world.shape[0] >= 4 and world.shape == image.shape and world.shape[1] == 2\n",
        "\n",
        "\n",
        "    # Build design matrix A (2 rows per correspondence)\n",
        "    N = world.shape[0]\n",
        "    A = []\n",
        "    for i in range(N):\n",
        "        # X_i, Y_i = X_n[i]\n",
        "        # u_i, v_i = x_n[i]\n",
        "        X, Y = world[i]\n",
        "        u, v = image[i]\n",
        "        A.append([0, 0, 0, -X, -Y, -1, v*X, v*Y, v])\n",
        "        A.append([X, Y, 1, 0, 0, 0, -u*X, -u*Y, -u])\n",
        "    A = np.asarray(A, float)\n",
        "\n",
        "    # Solve Ah=0 via SVD (last singular vector)\n",
        "    U, S, Vt = np.linalg.svd(A)\n",
        "    h = Vt[-1, :]\n",
        "    H = h.reshape(3,3)\n",
        "\n",
        "    return H / H[-1, -1]\n",
        "\n",
        "# ---------- Pose from Homography (no OpenCV) ----------\n",
        "def pose_from_homography(K, model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Given intrinsics K and planar correspondences (X,Y,0) -> (u,v),\n",
        "    compute pose [R|t] from homography, WITHOUT using OpenCV.\n",
        "\n",
        "    Steps:\n",
        "      1) H from normalized DLT:  x ~ H X\n",
        "      2) B = K^{-1} H = [r1 r2 t] up to scale\n",
        "      3) lambda from ||b1|| and ||b2||; r1 = lam*b1, r2 = lam*b2, r3 = r1 x r2\n",
        "      4) Orthonormalize R via SVD; enforce det(R)=+1\n",
        "      5) t = lam*b3\n",
        "      6) Cheirality: flip sign so average depth of model points is positive\n",
        "    Returns: (R, t, H)\n",
        "    \"\"\"\n",
        "    K = np.asarray(K, float)\n",
        "    H = _homography_dlt(model_pts_2d, image_pts_2d)\n",
        "\n",
        "    # Remove intrinsics\n",
        "    Kinv = np.linalg.inv(K)\n",
        "    B = Kinv @ H\n",
        "\n",
        "    b1, b2, b3 = B[:,0], B[:,1], B[:,2]\n",
        "\n",
        "    # Scale lambda: use both columns for stability\n",
        "    n1 = np.linalg.norm(b1)\n",
        "    n2 = np.linalg.norm(b2)\n",
        "    lam = (n1 + n2) / 2 if (n1 + n2) > 1e-12 else 1.0\n",
        "\n",
        "    r1 = b1 / lam\n",
        "    r2 = b2 / lam\n",
        "    r3 = np.cross(r1, r2)\n",
        "\n",
        "    R_approx = np.stack([r1, r2, r3], axis=1)\n",
        "\n",
        "    # Orthonormalize (closest rotation) via SVD\n",
        "    U, _, Vt = np.linalg.svd(R_approx)\n",
        "    R = U @ Vt\n",
        "    if np.linalg.det(R) < 0:\n",
        "        # Proper rotation\n",
        "        U[:,-1] *= -1\n",
        "        R = U @ Vt\n",
        "\n",
        "    t = b3.reshape(3,) / lam\n",
        "\n",
        "    return R, t, H\n"
      ],
      "metadata": {
        "id": "tWilwsmEUhgn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenCV Pose (solvePnP)"
      ],
      "metadata": {
        "id": "MFByVTBlUpIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pose_from_pnp(K, dist, model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Uses solvePnP with planar points lifted to Z=0.\n",
        "    model_pts_2d: (N,2) -> becomes (N,3) with Z=0\n",
        "    image_pts_2d: (N,2)\n",
        "    \"\"\"\n",
        "    obj3d = np.hstack([np.asarray(model_pts_2d, float), np.zeros((len(model_pts_2d),1), float)])\n",
        "    img2d = np.asarray(image_pts_2d, float)\n",
        "\n",
        "    ok, rvec, tvec = cv2.solvePnP(obj3d, img2d, K, dist if dist is not None and len(dist)>0 else None, flags=cv2.SOLVEPNP_ITERATIVE)\n",
        "    if not ok:\n",
        "        raise ValueError(\"solvePnP failed\")\n",
        "\n",
        "    R, _ = cv2.Rodrigues(rvec)\n",
        "    # Ensure proper rotation\n",
        "    U, _, Vt = np.linalg.svd(R)\n",
        "    R = U @ Vt\n",
        "    if np.linalg.det(R) < 0:\n",
        "        U[:,-1] *= -1\n",
        "        R = U @ Vt\n",
        "\n",
        "    t = tvec.reshape(3,)\n",
        "    return R, t"
      ],
      "metadata": {
        "id": "oASs1cF8UpeQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projection + Overlay"
      ],
      "metadata": {
        "id": "mZwr8hLTYy4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_points_world_to_image(pts_w3d, K, dist, R, t):\n",
        "    rvec, _ = cv2.Rodrigues(R)\n",
        "    tvec = t.reshape(3,1)\n",
        "    pts_img, _ = cv2.projectPoints(np.asarray(pts_w3d, float), rvec, tvec, K, dist if dist is not None and len(dist)>0 else None)\n",
        "    return pts_img.reshape(-1,2)\n",
        "\n",
        "def overlay_axes(image_bgr, K, dist, R, t, axis_len=0.05):\n",
        "    \"\"\"\n",
        "    Draws XYZ axes at world origin (0,0,0) with length axis_len (meters).\n",
        "    \"\"\"\n",
        "    img = image_bgr.copy()\n",
        "    origin = np.array([[0,0,0]], dtype=float)\n",
        "    axes = np.array([[axis_len,0,0],\n",
        "                     [0,axis_len,0],\n",
        "                     [0,0,axis_len]], dtype=float)\n",
        "\n",
        "    origin_px = project_points_world_to_image(origin, K, dist, R, t)[0]\n",
        "    axes_px = project_points_world_to_image(axes, K, dist, R, t)\n",
        "\n",
        "    O = tuple(np.round(origin_px).astype(int))\n",
        "    X = tuple(np.round(axes_px[0]).astype(int))\n",
        "    Y = tuple(np.round(axes_px[1]).astype(int))\n",
        "    Z = tuple(np.round(axes_px[2]).astype(int))\n",
        "\n",
        "    cv2.line(img, O, X, (255,0,0), 2)\n",
        "    cv2.line(img, O, Y, (0,255,0), 2)\n",
        "    cv2.line(img, O, Z, (0,0,255), 2)\n",
        "    return img"
      ],
      "metadata": {
        "id": "EOPL8O08YzL3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D Camera Plot"
      ],
      "metadata": {
        "id": "0A8EfEsgZCrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_camera_poses_3d(\n",
        "    pose_list: List[Tuple[str, np.ndarray, np.ndarray]],\n",
        "    square_size_m: float\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize estimated camera centers in the chessboard/world frame.\n",
        "\n",
        "    Args:\n",
        "        pose_list: List of (image_path, rvec, tvec) for each image with a solved pose.\n",
        "                   rvec/tvec express the camera pose w.r.t. the board (world) frame.\n",
        "        square_size_m: Real-world size of one chessboard square (for axis/grid scaling).\n",
        "\n",
        "    Returns:\n",
        "        Plotly Figure with:\n",
        "          - A faint board grid lying on Z=0 (world XY plane)\n",
        "          - Camera centers as markers labeled #0, #1, â€¦\n",
        "          - Board axes (X, Y, Z) drawn at the origin\n",
        "\n",
        "    Notes:\n",
        "        - World frame convention (from earlier):\n",
        "            Board lies in Z=0; units are meters if `square_size_m` is in meters.\n",
        "        - Camera center is computed as C_w = -R^T * t (already done upstream).\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "    if len(pose_list) == 0:\n",
        "        # Nothing to plot â†’ keep consistent aspect mode so axes don't distort\n",
        "        fig.update_layout(title=\"No poses to display\", scene_aspectmode='data')\n",
        "        return fig\n",
        "\n",
        "\n",
        "    # Camera centers\n",
        "    centers, labels = [], []\n",
        "    for pth, rvec, tvec in pose_list:\n",
        "        C = Cam.camera_center_in_board(rvec, tvec)   # shape (3,)\n",
        "        centers.append(tvec.reshape((3,)))                            # keep as array/list; we'll cast below\n",
        "        labels.append(os.path.basename(pth))\n",
        "\n",
        "    # Cast to NumPy BEFORE any [:, ...] indexing\n",
        "    centers = np.asarray(centers, dtype=float)       # shape (N, 3)\n",
        "\n",
        "    # Flip Z so cameras in front of the board appear at +Z (matches overlay)\n",
        "    # centers[:, 2] *= -1\n",
        "\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=centers[:, 0], y=centers[:, 1], z=centers[:, 2],\n",
        "        mode=\"markers+text\",\n",
        "        text=[f\"#{i}\" for i in range(len(labels))],\n",
        "        textposition=\"top center\",\n",
        "        marker=dict(size=4), name=\"Camera centers\"\n",
        "    ))\n",
        "\n",
        "    # --- Board axes drawn at the origin (0,0,0) ---\n",
        "    L = 2.5 * 25.4*6/7  # axis length in world units\n",
        "    axes = {\n",
        "        \"X\": [[0, L], [0, 0], [0, 0]],\n",
        "        \"Y\": [[0, 0], [0, L], [0, 0]],\n",
        "        \"Z\": [[0, 0], [0, 0], [0, L]],\n",
        "    }\n",
        "    for name, (ax, ay, az) in axes.items():\n",
        "        fig.add_trace(go.Scatter3d(x=ax, y=ay, z=az, mode=\"lines\", name=f\"{name}-axis\"))\n",
        "\n",
        "    # Keep aspect ratio true to data scale; label axes with units\n",
        "    fig.update_layout(\n",
        "        title=\"Estimated Camera Poses w.r.t. Chessboard\",\n",
        "        scene=dict(\n",
        "            xaxis_title=\"X (m)\", yaxis_title=\"Y (m)\", zaxis_title=\"Z (m)\",\n",
        "            aspectmode=\"data\"\n",
        "        ),\n",
        "        legend=dict(x=0, y=1.0)\n",
        "    )\n",
        "    return fig"
      ],
      "metadata": {
        "id": "ljLi7Up-ZDR9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Bf7srHmeGbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ROBUST CHESSBOARD AUTO-DETECT (drop-in replacement for on_auto_detect + helpers) ---\n",
        "import cv2, numpy as np\n",
        "\n",
        "def _maybe_undistort(bgr, K, dist):\n",
        "    if K is None or dist is None or len(dist) == 0:\n",
        "        return bgr, K\n",
        "    h, w = bgr.shape[:2]\n",
        "    newK, _ = cv2.getOptimalNewCameraMatrix(K, dist, (w, h), 0)  # alpha=0 (crop)\n",
        "    und = cv2.undistort(bgr, K, dist, None, newK)\n",
        "    return und, newK\n",
        "\n",
        "def _prep_variants(bgr):\n",
        "    \"\"\"Generate a few variants to help detection.\"\"\"\n",
        "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
        "    v = []\n",
        "\n",
        "    # base\n",
        "    v.append(gray)\n",
        "\n",
        "    # equalized\n",
        "    v.append(cv2.equalizeHist(gray))\n",
        "\n",
        "    # mild blur\n",
        "    v.append(cv2.GaussianBlur(gray, (3,3), 0))\n",
        "\n",
        "    # slightly downscaled (sometimes helps SB)\n",
        "    small = cv2.resize(gray, None, fx=0.7, fy=0.7, interpolation=cv2.INTER_AREA)\n",
        "    v.append(small)\n",
        "\n",
        "    # inverted (white/black contrast reversal sometimes helps)\n",
        "    v.append(255 - gray)\n",
        "\n",
        "    return v\n",
        "\n",
        "def _try_find(gray, pattern_size):\n",
        "    flags = cv2.CALIB_CB_ADAPTIVE_THRESH | cv2.CALIB_CB_NORMALIZE_IMAGE\n",
        "    ok, corners = cv2.findChessboardCorners(gray, pattern_size, flags=flags)\n",
        "    if ok and corners is not None:\n",
        "        # refine on original scale only\n",
        "        if gray.ndim == 2 and gray.shape[0] >= 50 and gray.shape[1] >= 50:\n",
        "            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 60, 1e-6)\n",
        "            cv2.cornerSubPix(gray, corners, (5,5), (-1,-1), criteria)\n",
        "        return True, corners.reshape(-1,2).astype(np.float32)\n",
        "\n",
        "    # SB fallback (often more robust)\n",
        "    ok, corners = cv2.findChessboardCornersSB(gray, pattern_size, flags=0)\n",
        "    if ok and corners is not None:\n",
        "        return True, corners.reshape(-1,2).astype(np.float32)\n",
        "\n",
        "    return False, None\n"
      ],
      "metadata": {
        "id": "lSAuUS4GeG4z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pose Helpers"
      ],
      "metadata": {
        "id": "U_wvayxNvm_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Unified estimation callback: computes (R,t) from Homography and OpenCV, overlay, and 3D plot ===\n",
        "import io, json\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.figure import Figure\n",
        "\n",
        "def _pretty_matrix(M):\n",
        "    if M is None:\n",
        "        return \"None\"\n",
        "    return np.array2string(np.asarray(M), precision=5, suppress_small=True)\n",
        "\n",
        "def _normalize_corners(c):\n",
        "    \"\"\"\n",
        "    Accepts many shapes/types (tuple, list, Nx1x2, Nx2) and returns Nx2 float64 or None.\n",
        "    Also pulls the first array-like from tuples like (corners, meta).\n",
        "    \"\"\"\n",
        "    if c is None:\n",
        "        return None\n",
        "    if isinstance(c, tuple):\n",
        "        for item in c:\n",
        "            if hasattr(item, \"shape\") or isinstance(item, (list, tuple)):\n",
        "                c = item\n",
        "                break\n",
        "    c = np.array(c, dtype=np.float64)\n",
        "    if c.ndim == 3 and c.shape[1] == 1 and c.shape[2] == 2:\n",
        "        c = c.reshape(-1, 2)\n",
        "    if c.ndim == 2 and c.shape[1] == 2:\n",
        "        return c\n",
        "    return None\n",
        "\n",
        "def on_estimate_pose(image_bgr, intr_file, square_size_m, use_undistort, use_solvepnp_ransac=True):\n",
        "    # 0) Validate inputs\n",
        "    if image_bgr is None:\n",
        "        return \"No image\", \"\", \"No image\", \"\", None, None\n",
        "    if intr_file is None:\n",
        "        return \"No intrinsics\", \"\", \"No intrinsics\", \"\", None, None\n",
        "\n",
        "    # 1) Load intrinsics (support File object or string path)\n",
        "    try:\n",
        "        intr_path = getattr(intr_file, \"name\", None) or (intr_file if isinstance(intr_file, str) else None)\n",
        "        with open(intr_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        K = np.array(data.get(\"K\") or data.get(\"camera_matrix\"), dtype=float)\n",
        "        D_key = None\n",
        "        for k in [\"D\", \"distortion_coefficients\", \"distCoeffs\", \"dist\"]:\n",
        "            if k in data:\n",
        "                D_key = k\n",
        "                break\n",
        "        dist = np.array(data[D_key], dtype=float).reshape(-1,1) if D_key else np.zeros((5,1),dtype=float)\n",
        "    except Exception as e:\n",
        "        return f\"Bad intrinsics: {e}\", \"\", f\"Bad intrinsics: {e}\", \"\", None, None\n",
        "\n",
        "    # 2) Prepare image (undistort if requested)\n",
        "    bgr = image_bgr.copy()\n",
        "    if use_undistort:\n",
        "        h, w = bgr.shape[:2]\n",
        "        newK, _ = cv.getOptimalNewCameraMatrix(K, dist, (w,h), 0)\n",
        "        use_K = newK\n",
        "        use_dist = np.zeros((dist.size,1), dtype=float)\n",
        "    else:\n",
        "        use_K = K\n",
        "        use_dist = dist\n",
        "\n",
        "    # 3) Detect corners\n",
        "    corners = STATE['clicks']\n",
        "\n",
        "    corners = _normalize_corners(corners)\n",
        "    if corners is None or len(corners) < 4:\n",
        "        return \"No usable corners found\", \"\", \"No usable corners found\", \"\", None, None\n",
        "\n",
        "    # 4) Build world points\n",
        "    # objp = generate_checkerboard_points(int(rows), int(cols), float(square_size_m))  # (N,3)\n",
        "    objp = np.array([[0, 0, 0],[0, 25.4*6/7*6, 0],[25.4*6/7*9, 25.4*6/7*6, 0],[25.4*6/7*9, 0, 0]])\n",
        "\n",
        "    # 5a) Homography -> Pose (your API expects K, model_pts_2d, image_pts_2d)\n",
        "    try:\n",
        "        img_pts  = corners.astype(np.float64)           # shape (N,2)\n",
        "        wrld_pts = objp[:, :2].astype(np.float64)       # (X,Y) only\n",
        "        R_H, t_H, H = pose_from_homography(use_K, wrld_pts, img_pts)\n",
        "        h_R_txt = _pretty_matrix(R_H)\n",
        "        h_t_txt = _pretty_matrix(t_H.reshape(3,1))\n",
        "    except Exception as e:\n",
        "        h_R_txt = f\"Homography failed: {e}\"\n",
        "        h_t_txt = \"\"\n",
        "\n",
        "\n",
        "    # 5b) OpenCV PnP -> Pose (robust to type/shape/count)\n",
        "    cv_R_txt, cv_t_txt = \"solvePnP failed\", \"\"\n",
        "    R_cv, tvec = None, None\n",
        "    try:\n",
        "        # Ensure float32 and contiguous for OpenCV\n",
        "        obj_pts = np.ascontiguousarray(objp.astype(np.float32).reshape(-1,3))   # (N,1,3)\n",
        "        img_pts = np.ascontiguousarray(corners.astype(np.float32).reshape(-1,2))# (N,1,2)\n",
        "\n",
        "        # Enforce equal count and at least 4 points\n",
        "        N = min(len(obj_pts), len(img_pts))\n",
        "        obj_pts, img_pts = obj_pts[:N], img_pts[:N]\n",
        "        if N >= 4:\n",
        "            cv_success, rvec, tvec = cv.solvePnP(\n",
        "                obj_pts, img_pts,\n",
        "                use_K.astype(np.float64),\n",
        "                use_dist.astype(np.float64),\n",
        "                flags=cv.SOLVEPNP_ITERATIVE\n",
        "            )\n",
        "            if cv_success:\n",
        "                R_cv, _ = cv.Rodrigues(rvec)\n",
        "                cv_R_txt = _pretty_matrix(R_cv)\n",
        "                cv_t_txt = _pretty_matrix(tvec.reshape(3,1))\n",
        "        else:\n",
        "            cv_R_txt = f\"solvePnP needs â‰¥ 4 points; only got {N}\"\n",
        "            cv_t_txt = \"\"\n",
        "    except Exception as e:\n",
        "        cv_R_txt = f\"OpenCV failed: {e}\"\n",
        "        cv_t_txt = \"\"\n",
        "\n",
        "\n",
        "\n",
        "    # 6) Overlay reprojection sanity check\n",
        "    try:\n",
        "        use_R = R_cv if R_cv is not None else (R_H if 'R_H' in locals() else None)\n",
        "        use_t = tvec.reshape(3,1) if tvec is not None else (t_H.reshape(3,1) if 't_H' in locals() else None)\n",
        "        overlay = bgr.copy()\n",
        "        if use_R is not None and use_t is not None:\n",
        "            overlay = overlay_axes(overlay, use_K, use_dist, use_R, use_t, axis_len=float(square_size_m)*3.0)\n",
        "        if use_undistort:\n",
        "            overlay = cv.undistort(overlay, K, dist, None, newK)\n",
        "        overlay_rgb = cv.cvtColor(overlay, cv.COLOR_BGR2RGB)\n",
        "    except Exception:\n",
        "        overlay_rgb = None\n",
        "\n",
        "    # 7) 3D plot â€” your plotly-based function expects: [(label, rvec, tvec)], square_size_m\n",
        "    try:\n",
        "        entries = []\n",
        "\n",
        "        # OpenCV pose (if available)\n",
        "        if R_cv is not None and tvec is not None:\n",
        "            # If rvec exists from solvePnP, use it; otherwise derive from R_cv\n",
        "            rvec_use = rvec if 'rvec' in locals() and rvec is not None else cv.Rodrigues(R_cv)[0]\n",
        "            entries.append((\n",
        "                \"OpenCV\",\n",
        "                np.asarray(rvec_use, dtype=np.float64).reshape(3,1),\n",
        "                np.asarray(tvec, dtype=np.float64).reshape(3,1),\n",
        "            ))\n",
        "\n",
        "        # Homography pose (if available)\n",
        "        if 'R_H' in locals() and R_H is not None and 't_H' in locals() and t_H is not None:\n",
        "            rvec_H = cv.Rodrigues(R_H)[0]\n",
        "            entries.append((\n",
        "                \"Homography\",\n",
        "                np.asarray(rvec_H, dtype=np.float64).reshape(3,1),\n",
        "                np.asarray(t_H, dtype=np.float64).reshape(3,1),\n",
        "            ))\n",
        "\n",
        "        if entries:\n",
        "            fig = plot_camera_poses_3d(entries, float(square_size_m))\n",
        "        else:\n",
        "            # Visible placeholder so the panel never looks empty\n",
        "            fig = go.Figure()\n",
        "            fig.update_layout(title=\"No pose available (Homography & OpenCV both missing)\", height=320)\n",
        "    except Exception as e:\n",
        "        # Show the error in the plot panel instead of silently returning None\n",
        "        fig = go.Figure()\n",
        "        fig.add_annotation(text=f\"3D plot error: {e}\", x=0.5, y=0.5, showarrow=False)\n",
        "        fig.update_layout(title=\"3D Camera Pose (error)\", height=320)\n",
        "\n",
        "\n",
        "    return h_R_txt, h_t_txt, cv_R_txt, cv_t_txt, cv2.cvtColor(overlay_rgb, cv2.COLOR_BGR2RGB), fig\n"
      ],
      "metadata": {
        "id": "J-aSN6K2vnom"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimal Gradio UI (ordered clicks + undo)"
      ],
      "metadata": {
        "id": "BcNKBvNFZMSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --- State ---\n",
        "STATE = {\n",
        "    \"img_bgr\": None,\n",
        "    \"display_img_bgr\": None,\n",
        "    \"clicks\": [],\n",
        "    \"K\": None,\n",
        "    \"distCoeffs\": None,\n",
        "    \"model_pts_2d\": None\n",
        "}\n",
        "\n",
        "# --- Load Image ---\n",
        "def on_image_upload(img):\n",
        "    if img is None:\n",
        "        return None, \"No image\"\n",
        "    bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    STATE[\"img_bgr\"] = bgr\n",
        "\n",
        "    (h, w) = bgr.shape[:2]\n",
        "    desired_height = 800\n",
        "    ratio = desired_height / float(h)\n",
        "    new_width_aspect = int(w * ratio)\n",
        "    resized_image = cv2.resize(bgr, (new_width_aspect, desired_height))\n",
        "\n",
        "    STATE[\"display_img_bgr\"] = resized_image\n",
        "    STATE[\"clicks\"].clear()\n",
        "    return cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB), \"Image loaded\"\n",
        "\n",
        "# --- Load Intrinsics ---\n",
        "def on_intrinsics_upload(file_obj):\n",
        "    if file_obj is None:\n",
        "        return \"No intrinsics file.\"\n",
        "    K, dist = load_intrinsics(file_obj.name)\n",
        "    STATE[\"K\"] = K\n",
        "    STATE[\"distCoeffs\"] = dist\n",
        "    return f\"Loaded K and distCoeffs...\\n\\nK = \\n{K}\\n\\ndistCoeffs = {dist}\"\n",
        "\n",
        "def draw_outlined_text(img, text, pos, font, font_size, font_color, text_size, outline_thickness):\n",
        "    outline_color = (255, 255, 255)\n",
        "    if sum(font_color) >= 255:\n",
        "        ouline_color = (0, 0, 0)\n",
        "    cv2.putText(img, text, pos, font, font_size, ouline_color, text_size + outline_thickness)\n",
        "    cv2.putText(img, text, pos, font, font_size, font_color, text_size)\n",
        "\n",
        "# --- Manual Click Handler ---\n",
        "def on_click(evt: gr.SelectData):\n",
        "    STATE[\"clicks\"].append((evt.index[0], evt.index[1]))\n",
        "    vis = STATE[\"display_img_bgr\"].copy()\n",
        "    for i, (x,y) in enumerate(STATE[\"clicks\"]):\n",
        "        cv2.circle(vis, (int(x),int(y)), 5, (0,255,0), -1)\n",
        "        draw_outlined_text(vis, str(i+1), (int(x)+5,int(y)-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,255,255), 3, 2)\n",
        "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB), f\"Clicks: {len(STATE['clicks'])}\"\n",
        "\n",
        "def clear_clicks():\n",
        "  STATE[\"clicks\"].clear()\n",
        "  return \"Clicks Cleared\", cv2.cvtColor(STATE[\"display_img_bgr\"], cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Pose from a Planar Object â€” Manual or Auto Points\")\n",
        "\n",
        "    with gr.Row():\n",
        "        img = gr.Image(type=\"numpy\", label=\"Upload planar scene\")\n",
        "        img_status = gr.Textbox(label=\"Image Status:\", value=\"Awaiting image upload...\", interactive=False)\n",
        "        intr = gr.File(label=\"Upload intrinsics JSON\")\n",
        "        status_intr = gr.Textbox(label=\"Intrinsics Status:\",value=\"Awaiting JSON upload...\", interactive=False)\n",
        "        img.upload(on_image_upload, img, [img, img_status])\n",
        "        img.clear(lambda: \"Awaiting image upload...\", outputs=img_status)\n",
        "        intr.upload(on_intrinsics_upload, intr, status_intr)\n",
        "        intr.clear(lambda: \"Awaiting JSON upload...\", outputs=status_intr)\n",
        "\n",
        "    click_info = gr.Textbox(label=\"Click info\")\n",
        "    clear = gr.Button(\"Clear Clicks\")\n",
        "\n",
        "    # manual clicks happen only here\n",
        "    img.select(on_click, None, [img, click_info])\n",
        "    clear.click(clear_clicks, None, [click_info, img])\n",
        "\n",
        "    # --- Pose estimation controls & outputs ---\n",
        "    with gr.Row():\n",
        "        sq_in = gr.Number(label=\"Square size (meters)\", value=25.4*6/7, precision=6)\n",
        "        use_undistort = gr.Checkbox(label=\"Undistort image before estimation\", value=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        btn_estimate = gr.Button(\"Estimate Pose\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        h_R = gr.Textbox(label=\"Homography â†’ Pose: R\", lines=6, interactive=False)\n",
        "        h_t = gr.Textbox(label=\"Homography â†’ Pose: t\", lines=3, interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        cv_R = gr.Textbox(label=\"OpenCV (solvePnP): R\", lines=6, interactive=False)\n",
        "        cv_t = gr.Textbox(label=\"OpenCV (solvePnP): t\", lines=3, interactive=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        overlay_img = gr.Image(label=\"Overlay: reprojected axes / points\", type=\"numpy\")\n",
        "        pose_fig = gr.Plot(label=\"3D Camera Pose (world frame)\")\n",
        "\n",
        "    # Wire up the click: uses shared inputs img and intr from above\n",
        "    btn_estimate.click(\n",
        "        fn=on_estimate_pose,\n",
        "        inputs=[img, intr, sq_in, use_undistort],\n",
        "        outputs=[h_R, h_t, cv_R, cv_t, overlay_img, pose_fig]\n",
        "    )"
      ],
      "metadata": {
        "id": "ckC6nCsMZM2i"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(inline=True)"
      ],
      "metadata": {
        "id": "UDPBlupBkOXg",
        "outputId": "386277e2-0096-4341-be8a-d30b961c4322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8d9c9c471cdce2a919.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8d9c9c471cdce2a919.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison Notes\n",
        "\n",
        "- The rotation matrices (R) from both methods were broadly consistent in orientation, though not identical.  \n",
        "- The translation vectors (t) differed mainly in scale: homography gave values around [-640, -747, 1671], while OpenCV gave roughly double that scale at [-1531, -1759, 4063]. This is expected since homography only recovers pose up to scale.  \n",
        "- Pose estimation was very sensitive to point ordering. Using more than 4 points and applying RANSAC improved robustness against noisy or mis-clicked points.  \n",
        "- Undistorting the image before estimation produced overlays that matched the board more closely. Using distortion coefficients directly also worked, but consistency across both pipelines was essential.  \n",
        "- The homography-based pose required re-orthonormalization of R and checking the determinant to ensure a proper rotation matrix. OpenCVâ€™s `solvePnP` handled this automatically, leading to more stable results.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wg7xru8-fn-R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLU3bjAs4i8z"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}
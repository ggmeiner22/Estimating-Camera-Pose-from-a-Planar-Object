{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggmeiner22/Estimating-Camera-Pose-from-a-Planar-Object/blob/main/EstimatingCameraPoseFromAPlanarObject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal\n",
        "\n",
        "Given a calibrated camera (i.e., known intrinsics **K**) and a single image of a planar object, estimate the camera **pose** (i.e., extrinsics **R**, **t**).\n",
        "\n",
        "I will:\n",
        "\n",
        "1. build a Gradio UI to click 2D features and manage point order,\n",
        "2. estimate pose **from a homography** (explicit derivation), and\n",
        "3. estimate pose using **OpenCV** functions,\n",
        "4. compare the two (short notes only)."
      ],
      "metadata": {
        "id": "EpbqkAzHIM5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs dependencies"
      ],
      "metadata": {
        "id": "CWn5jpoaI94w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B2u8_q2_IJ7l"
      },
      "outputs": [],
      "source": [
        "%pip -q install opencv-python numpy gradio matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "SqNF6-vkLfzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, io, math\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "from matplotlib.figure import Figure"
      ],
      "metadata": {
        "id": "yyqTxE69LgHW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 1) Data and Calibration\n",
        "\n"
      ],
      "metadata": {
        "id": "_cIm1l6Ni42v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Download demo images"
      ],
      "metadata": {
        "id": "hEf6V6uEgjGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Penguini128/computer-vision-demo-images"
      ],
      "metadata": {
        "id": "etvfda2RgjVy",
        "outputId": "c75ee18b-86ae-43dc-fe1e-1307555e6f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'computer-vision-demo-images' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-generate and save intrinsic parameters for demo locally"
      ],
      "metadata": {
        "id": "pkvTxREDg1kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKERBOARD = (9, 6)   # number of inner corners per a chessboard row and column\n",
        "SQUARE_SIZE = 25.4*6/7       # set to real size of a square if you want results in real units\n",
        "IMAGE_DIR = \"computer-vision-demo-images/*.jpeg\"  # path to your images\n",
        "SAVE_FILE = \"camera_params.npz\"\n",
        "\n",
        "# Create 3D points for the checkerboard corners, e.g. (0,0,0), (1,0,0), ...\n",
        "objp = np.zeros((CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\n",
        "objp[:, :2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\n",
        "# Scale points by SQUARE_SIZE\n",
        "objp *= SQUARE_SIZE\n",
        "\n",
        "# Arrays to store object points and image points\n",
        "objpoints = []  # 3D points in real world space\n",
        "imgpoints = []  # 2D points in image plane\n",
        "\n",
        "images = glob.glob(IMAGE_DIR)\n",
        "\n",
        "for fname in images:\n",
        "    img = cv2.imread(fname)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find the chess board corners\n",
        "    ret, corners = cv2.findChessboardCorners(gray, CHECKERBOARD, None)\n",
        "\n",
        "    if ret:\n",
        "        objpoints.append(objp)\n",
        "        # refine corner locations\n",
        "        corners2 = cv2.cornerSubPix(\n",
        "            gray, corners, (11, 11), (-1, -1),\n",
        "            criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "        )\n",
        "        imgpoints.append(corners2)\n",
        "\n",
        "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n",
        "    objpoints, imgpoints, gray.shape[::-1], None, None\n",
        ")\n",
        "\n",
        "data = {\n",
        "    'K' : mtx.tolist(),\n",
        "    'distCoeffs' : dist.tolist()\n",
        "}\n",
        "\n",
        "print(data)\n",
        "\n",
        "\n",
        "with open('intrinsics.json', 'w', encoding='utf-8') as f:\n",
        "  json.dump(data, f, indent=4)\n"
      ],
      "metadata": {
        "id": "-pJE1d7Fg068",
        "outputId": "673b37d2-0267-40f3-e100-0c137b6269a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'K': [[3139.0013441043434, 0.0, 1545.6609719550318], [0.0, 3432.1716181680863, 1956.7901560536493], [0.0, 0.0, 1.0]], 'distCoeffs': [[0.09632586982681611, 0.22821887323503123, -0.021507721156426254, 0.0020577528616761765, -1.5481524614694138]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intrinsics + Model Points"
      ],
      "metadata": {
        "id": "Dlz-Vz3EPmIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_intrinsics(json_path):\n",
        "    \"\"\"\n",
        "    Expect JSON with keys:\n",
        "      K: [[fx,0,cx],[0,fy,cy],[0,0,1]]\n",
        "      distCoeffs: [k1,k2,p1,p2,k3] (length 4,5,8, or 12 acceptable)\n",
        "    \"\"\"\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    K = np.array(data[\"K\"], dtype=float)\n",
        "    dist = np.array(data.get(\"distCoeffs\", []), dtype=float).reshape(-1,1) if data.get(\"distCoeffs\") is not None else None\n",
        "    return K, dist\n",
        "\n",
        "def generate_checkerboard_points(cols, rows, square_size):\n",
        "    \"\"\"\n",
        "    Returns planar model points (Z=0) in meters in row-major order of INNER corners.\n",
        "    cols = number of inner corners along x, rows along y.\n",
        "    \"\"\"\n",
        "    xs, ys = np.meshgrid(np.arange(cols), np.arange(rows))\n",
        "    pts = np.stack([xs.ravel(), ys.ravel()], axis=1).astype(float) * float(square_size)\n",
        "    return pts  # shape: (N,2), Z=0 implied\n"
      ],
      "metadata": {
        "id": "Di8gwMiiPmcx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pose from a homography (explicit derivation)"
      ],
      "metadata": {
        "id": "_hlEjGKPUDMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---------- Utilities: normalized DLT for Homography ----------\n",
        "def _normalize_points_2d(pts):\n",
        "    \"\"\"\n",
        "    Hartley normalization: translate to zero mean and scale so that\n",
        "    the mean Euclidean distance from the origin is sqrt(2).\n",
        "    Returns (T, pts_norm), where T is the 3x3 similarity transform.\n",
        "    \"\"\"\n",
        "    pts = np.asarray(pts, float)\n",
        "    assert pts.shape[1] == 2\n",
        "    mean = pts.mean(axis=0)\n",
        "    pts_c = pts - mean\n",
        "    mean_dist = np.mean(np.sqrt(np.sum(pts_c**2, axis=1)))\n",
        "    s = np.sqrt(2) / mean_dist if mean_dist > 0 else 1.0\n",
        "\n",
        "    T = np.array([\n",
        "        [s, 0, -s*mean[0]],\n",
        "        [0, s, -s*mean[1]],\n",
        "        [0, 0, 1.0]\n",
        "    ], dtype=float)\n",
        "\n",
        "    pts_h = np.hstack([pts, np.ones((len(pts),1))])\n",
        "    pts_n = (T @ pts_h.T).T\n",
        "    return T, pts_n[:, :2]\n",
        "\n",
        "def _homography_dlt(model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Compute H (3x3) such that  [u v 1]^T ~ H [X Y 1]^T\n",
        "    using normalized DLT (no OpenCV).\n",
        "    \"\"\"\n",
        "    X = np.asarray(model_pts_2d, float)\n",
        "    x = np.asarray(image_pts_2d, float)\n",
        "    assert X.shape[0] >= 4 and X.shape == x.shape and X.shape[1] == 2\n",
        "\n",
        "    # Normalize both sets\n",
        "    T_x, x_n = _normalize_points_2d(x)\n",
        "    T_X, X_n = _normalize_points_2d(X)\n",
        "\n",
        "    # Build design matrix A (2 rows per correspondence)\n",
        "    N = X.shape[0]\n",
        "    A = []\n",
        "    for i in range(N):\n",
        "        X_i, Y_i = X_n[i]\n",
        "        u_i, v_i = x_n[i]\n",
        "        A.append([0, 0, 0, -X_i, -Y_i, -1, v_i*X_i, v_i*Y_i, v_i])\n",
        "        A.append([X_i, Y_i, 1, 0, 0, 0, -u_i*X_i, -u_i*Y_i, -u_i])\n",
        "    A = np.asarray(A, float)\n",
        "\n",
        "    # Solve Ah=0 via SVD (last singular vector)\n",
        "    U, S, Vt = np.linalg.svd(A)\n",
        "    h = Vt[-1, :]\n",
        "    Hn = h.reshape(3,3)\n",
        "\n",
        "    # Denormalize: x ~ T_x^{-1} * Hn * T_X * X\n",
        "    H = np.linalg.inv(T_x) @ Hn @ T_X\n",
        "    # Scale so that H[2,2] = 1 (optional; avoids huge scale)\n",
        "    if abs(H[2,2]) > 1e-12:\n",
        "        H = H / H[2,2]\n",
        "    return H\n",
        "\n",
        "# ---------- Pose from Homography (no OpenCV) ----------\n",
        "def pose_from_homography(K, model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Given intrinsics K and planar correspondences (X,Y,0) -> (u,v),\n",
        "    compute pose [R|t] from homography, WITHOUT using OpenCV.\n",
        "\n",
        "    Steps:\n",
        "      1) H from normalized DLT:  x ~ H X\n",
        "      2) B = K^{-1} H = [r1 r2 t] up to scale\n",
        "      3) lambda from ||b1|| and ||b2||; r1 = lam*b1, r2 = lam*b2, r3 = r1 x r2\n",
        "      4) Orthonormalize R via SVD; enforce det(R)=+1\n",
        "      5) t = lam*b3\n",
        "      6) Cheirality: flip sign so average depth of model points is positive\n",
        "    Returns: (R, t, H)\n",
        "    \"\"\"\n",
        "    K = np.asarray(K, float)\n",
        "    H = _homography_dlt(model_pts_2d, image_pts_2d)\n",
        "\n",
        "    # Remove intrinsics\n",
        "    Kinv = np.linalg.inv(K)\n",
        "    B = Kinv @ H\n",
        "\n",
        "    b1, b2, b3 = B[:,0], B[:,1], B[:,2]\n",
        "\n",
        "    # Scale lambda: use both columns for stability\n",
        "    n1 = np.linalg.norm(b1)\n",
        "    n2 = np.linalg.norm(b2)\n",
        "    lam = 2.0 / (n1 + n2) if (n1 + n2) > 1e-12 else 1.0\n",
        "\n",
        "    r1 = lam * b1\n",
        "    r2 = lam * b2\n",
        "    r3 = np.cross(r1, r2)\n",
        "\n",
        "    R_approx = np.stack([r1, r2, r3], axis=1)\n",
        "\n",
        "    # Orthonormalize (closest rotation) via SVD\n",
        "    U, _, Vt = np.linalg.svd(R_approx)\n",
        "    R = U @ Vt\n",
        "    if np.linalg.det(R) < 0:\n",
        "        # Proper rotation\n",
        "        U[:,-1] *= -1\n",
        "        R = U @ Vt\n",
        "\n",
        "    t = lam * b3.reshape(3,)\n",
        "\n",
        "    # Cheirality: ensure points lie in front (positive depth)\n",
        "    # Depth for a world point [X,Y,0,1]^T is: z = (R[2,:] @ [X,Y,0]) + t[2]\n",
        "    X = np.asarray(model_pts_2d, float)\n",
        "    depths = (R[2,0]*X[:,0] + R[2,1]*X[:,1] + t[2])\n",
        "    if np.mean(depths) < 0:\n",
        "        # Flip sign (projectively equivalent)\n",
        "        R[:,0:2] *= -1\n",
        "        t *= -1\n",
        "        # Re-orthonormalize after flip just in case\n",
        "        U, _, Vt = np.linalg.svd(R)\n",
        "        R = U @ Vt\n",
        "        if np.linalg.det(R) < 0:\n",
        "            U[:,-1] *= -1\n",
        "            R = U @ Vt\n",
        "\n",
        "    return R, t, H\n"
      ],
      "metadata": {
        "id": "tWilwsmEUhgn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenCV Pose (solvePnP)"
      ],
      "metadata": {
        "id": "MFByVTBlUpIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pose_from_pnp(K, dist, model_pts_2d, image_pts_2d):\n",
        "    \"\"\"\n",
        "    Uses solvePnP with planar points lifted to Z=0.\n",
        "    model_pts_2d: (N,2) -> becomes (N,3) with Z=0\n",
        "    image_pts_2d: (N,2)\n",
        "    \"\"\"\n",
        "    obj3d = np.hstack([np.asarray(model_pts_2d, float), np.zeros((len(model_pts_2d),1), float)])\n",
        "    img2d = np.asarray(image_pts_2d, float)\n",
        "\n",
        "    ok, rvec, tvec = cv2.solvePnP(obj3d, img2d, K, dist if dist is not None and len(dist)>0 else None, flags=cv2.SOLVEPNP_ITERATIVE)\n",
        "    if not ok:\n",
        "        raise ValueError(\"solvePnP failed\")\n",
        "\n",
        "    R, _ = cv2.Rodrigues(rvec)\n",
        "    # Ensure proper rotation\n",
        "    U, _, Vt = np.linalg.svd(R)\n",
        "    R = U @ Vt\n",
        "    if np.linalg.det(R) < 0:\n",
        "        U[:,-1] *= -1\n",
        "        R = U @ Vt\n",
        "\n",
        "    t = tvec.reshape(3,)\n",
        "    return R, t"
      ],
      "metadata": {
        "id": "oASs1cF8UpeQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Projection + Overlay"
      ],
      "metadata": {
        "id": "mZwr8hLTYy4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_points_world_to_image(pts_w3d, K, dist, R, t):\n",
        "    rvec, _ = cv2.Rodrigues(R)\n",
        "    tvec = t.reshape(3,1)\n",
        "    pts_img, _ = cv2.projectPoints(np.asarray(pts_w3d, float), rvec, tvec, K, dist if dist is not None and len(dist)>0 else None)\n",
        "    return pts_img.reshape(-1,2)\n",
        "\n",
        "def overlay_axes(image_bgr, K, dist, R, t, axis_len=0.05):\n",
        "    \"\"\"\n",
        "    Draws XYZ axes at world origin (0,0,0) with length axis_len (meters).\n",
        "    \"\"\"\n",
        "    img = image_bgr.copy()\n",
        "    origin = np.array([[0,0,0]], dtype=float)\n",
        "    axes = np.array([[axis_len,0,0],\n",
        "                     [0,axis_len,0],\n",
        "                     [0,0,axis_len]], dtype=float)\n",
        "\n",
        "    origin_px = project_points_world_to_image(origin, K, dist, R, t)[0]\n",
        "    axes_px = project_points_world_to_image(axes, K, dist, R, t)\n",
        "\n",
        "    O = tuple(np.round(origin_px).astype(int))\n",
        "    X = tuple(np.round(axes_px[0]).astype(int))\n",
        "    Y = tuple(np.round(axes_px[1]).astype(int))\n",
        "    Z = tuple(np.round(axes_px[2]).astype(int))\n",
        "\n",
        "    cv2.line(img, O, X, (255,0,0), 2)\n",
        "    cv2.line(img, O, Y, (0,255,0), 2)\n",
        "    cv2.line(img, O, Z, (0,0,255), 2)\n",
        "    return img"
      ],
      "metadata": {
        "id": "EOPL8O08YzL3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D Camera Plot"
      ],
      "metadata": {
        "id": "0A8EfEsgZCrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_camera_poses_3d(poses, plane='z=0'):\n",
        "    \"\"\"\n",
        "    poses: list of dicts with {'R': (3,3), 't': (3,), 'label': str}\n",
        "    Shows camera centers in world frame. Camera center C = -R^T t\n",
        "    \"\"\"\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # draw plane z=0\n",
        "    if plane == 'z=0':\n",
        "        xx = np.linspace(-0.2, 0.2, 2)\n",
        "        yy = np.linspace(-0.2, 0.2, 2)\n",
        "        XX, YY = np.meshgrid(xx, yy)\n",
        "        ZZ = np.zeros_like(XX)\n",
        "        ax.plot_surface(XX, YY, ZZ, alpha=0.2)\n",
        "\n",
        "    for p in poses:\n",
        "        R = p['R']; t = p['t']\n",
        "        C = -R.T @ t\n",
        "        ax.scatter(C[0], C[1], C[2], s=40)\n",
        "        ax.text(C[0], C[1], C[2], p.get('label','cam'))\n",
        "\n",
        "        # draw camera basis\n",
        "        s = 0.03\n",
        "        axes_w = R.T @ (np.eye(3) * s)  # camera axes in world\n",
        "        o = C\n",
        "        ax.plot([o[0], o[0]+axes_w[0,0]],[o[1], o[1]+axes_w[1,0]],[o[2], o[2]+axes_w[2,0]])\n",
        "        ax.plot([o[0], o[0]+axes_w[0,1]],[o[1], o[1]+axes_w[1,1]],[o[2], o[2]+axes_w[2,1]])\n",
        "        ax.plot([o[0], o[0]+axes_w[0,2]],[o[1], o[1]+axes_w[1,2]],[o[2], o[2]+axes_w[2,2]])\n",
        "\n",
        "    ax.set_xlabel('X (m)'); ax.set_ylabel('Y (m)'); ax.set_zlabel('Z (m)')\n",
        "    ax.set_box_aspect([1,1,0.5])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ljLi7Up-ZDR9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Bf7srHmeGbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ROBUST CHESSBOARD AUTO-DETECT (drop-in replacement for on_auto_detect + helpers) ---\n",
        "import cv2, numpy as np\n",
        "\n",
        "def _maybe_undistort(bgr, K, dist):\n",
        "    if K is None or dist is None or len(dist) == 0:\n",
        "        return bgr, K\n",
        "    h, w = bgr.shape[:2]\n",
        "    newK, _ = cv2.getOptimalNewCameraMatrix(K, dist, (w, h), 0)  # alpha=0 (crop)\n",
        "    und = cv2.undistort(bgr, K, dist, None, newK)\n",
        "    return und, newK\n",
        "\n",
        "def _prep_variants(bgr):\n",
        "    \"\"\"Generate a few variants to help detection.\"\"\"\n",
        "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
        "    v = []\n",
        "\n",
        "    # base\n",
        "    v.append(gray)\n",
        "\n",
        "    # equalized\n",
        "    v.append(cv2.equalizeHist(gray))\n",
        "\n",
        "    # mild blur\n",
        "    v.append(cv2.GaussianBlur(gray, (3,3), 0))\n",
        "\n",
        "    # slightly downscaled (sometimes helps SB)\n",
        "    small = cv2.resize(gray, None, fx=0.7, fy=0.7, interpolation=cv2.INTER_AREA)\n",
        "    v.append(small)\n",
        "\n",
        "    # inverted (white/black contrast reversal sometimes helps)\n",
        "    v.append(255 - gray)\n",
        "\n",
        "    return v\n",
        "\n",
        "def _try_find(gray, pattern_size):\n",
        "    flags = cv2.CALIB_CB_ADAPTIVE_THRESH | cv2.CALIB_CB_NORMALIZE_IMAGE\n",
        "    ok, corners = cv2.findChessboardCorners(gray, pattern_size, flags=flags)\n",
        "    if ok and corners is not None:\n",
        "        # refine on original scale only\n",
        "        if gray.ndim == 2 and gray.shape[0] >= 50 and gray.shape[1] >= 50:\n",
        "            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 60, 1e-6)\n",
        "            cv2.cornerSubPix(gray, corners, (5,5), (-1,-1), criteria)\n",
        "        return True, corners.reshape(-1,2).astype(np.float32)\n",
        "\n",
        "    # SB fallback (often more robust)\n",
        "    ok, corners = cv2.findChessboardCornersSB(gray, pattern_size, flags=0)\n",
        "    if ok and corners is not None:\n",
        "        return True, corners.reshape(-1,2).astype(np.float32)\n",
        "\n",
        "    return False, None\n",
        "\n",
        "def robust_auto_detect(bgr, cols, rows, K=None, dist=None, detect_on_undistorted=False):\n",
        "    \"\"\"\n",
        "    Tries: raw/undistorted, multiple image variants, and a couple of nearby pattern sizes.\n",
        "    Returns (ok, corners, used_cols, used_rows, used_K)\n",
        "    \"\"\"\n",
        "    # Candidate inner-corner sizes (try requested first, then neighbors)\n",
        "    sizes = [(int(cols), int(rows))]\n",
        "    # Add a couple of nearby guesses (common off-by-one)\n",
        "    neighbor_sizes = {(int(cols)-1, int(rows)),\n",
        "                      (int(cols), int(rows)-1),\n",
        "                      (int(cols)+1, int(rows)),\n",
        "                      (int(cols), int(rows)+1)}\n",
        "    sizes += [s for s in neighbor_sizes if s[0] > 2 and s[1] > 2]\n",
        "\n",
        "    # Choose raw vs undistorted for detection\n",
        "    if detect_on_undistorted:\n",
        "        work_bgr, work_K = _maybe_undistort(bgr, K, dist)\n",
        "    else:\n",
        "        work_bgr, work_K = bgr, K\n",
        "\n",
        "    variants = _prep_variants(work_bgr)\n",
        "    for (c,r) in sizes:\n",
        "        pat = (c, r)\n",
        "        for g in variants:\n",
        "            ok, corners = _try_find(g, pat)\n",
        "            if ok:\n",
        "                # If we detected on a resized 'small', rescale back up\n",
        "                if g.shape[:2] != cv2.cvtColor(work_bgr, cv2.COLOR_BGR2GRAY).shape[:2]:\n",
        "                    sy = work_bgr.shape[0] / g.shape[0]\n",
        "                    sx = work_bgr.shape[1] / g.shape[1]\n",
        "                    corners = corners * np.array([sx, sy], dtype=np.float32)\n",
        "                return True, corners, c, r, work_K\n",
        "    return False, None, int(cols), int(rows), work_K\n",
        "\n",
        "def on_auto_detect(cols, rows, detect_on_undistorted=True):\n",
        "    \"\"\"\n",
        "    Gradio callback: fills CLICK_STATE['clicks'] with detected corners in row-major order.\n",
        "    \"\"\"\n",
        "    if CLICK_STATE[\"img_bgr\"] is None:\n",
        "        return None, \"No image uploaded.\"\n",
        "\n",
        "    bgr = CLICK_STATE[\"img_bgr\"]\n",
        "    K = CLICK_STATE[\"K\"]\n",
        "    dist = CLICK_STATE[\"dist\"]\n",
        "\n",
        "    ok, corners, used_c, used_r, usedK = robust_auto_detect(\n",
        "        bgr, cols, rows, K=K, dist=dist, detect_on_undistorted=bool(detect_on_undistorted)\n",
        "    )\n",
        "    if not ok:\n",
        "        return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), f\"Auto-detect failed after robust tries (asked {cols}×{rows}). Tip: verify inner-corner counts.\"\n",
        "\n",
        "    # Push into click list\n",
        "    CLICK_STATE[\"clicks\"] = [(float(x), float(y)) for (x,y) in corners]\n",
        "\n",
        "    # Visualize sparsely to avoid clutter\n",
        "    vis = bgr.copy()\n",
        "    for i,(x,y) in enumerate(CLICK_STATE[\"clicks\"]):\n",
        "        cv2.circle(vis, (int(x), int(y)), 4, (0,255,255), -1)\n",
        "        if i % (used_c//2 if used_c>6 else 4) == 0:\n",
        "            cv2.putText(vis, str(i), (int(x)+4, int(y)-4), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0,255,255), 1)\n",
        "\n",
        "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB), f\"Clicks: {len(CLICK_STATE['clicks'])} (detected {used_c}×{used_r})\"\n"
      ],
      "metadata": {
        "id": "lSAuUS4GeG4z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimal Gradio UI (ordered clicks + undo)"
      ],
      "metadata": {
        "id": "BcNKBvNFZMSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --- State ---\n",
        "CLICK_STATE = {\n",
        "    \"img_bgr\": None,\n",
        "    \"display_img_bgr\": None,\n",
        "    \"clicks\": [],\n",
        "    \"K\": None,\n",
        "    \"dist\": None,\n",
        "    \"model_pts_2d\": None\n",
        "}\n",
        "\n",
        "# --- Load Image ---\n",
        "def on_image_upload(img):\n",
        "    if img is None:\n",
        "        return None, \"No image\"\n",
        "    bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    CLICK_STATE[\"img_bgr\"] = bgr\n",
        "\n",
        "    (h, w) = bgr.shape[:2]\n",
        "    desired_height = 800\n",
        "    ratio = desired_height / float(h)\n",
        "    new_width_aspect = int(w * ratio)\n",
        "    resized_image = cv2.resize(bgr, (new_width_aspect, desired_height))\n",
        "\n",
        "    CLICK_STATE[\"display_img_bgr\"] = resized_image\n",
        "    CLICK_STATE[\"clicks\"].clear()\n",
        "    return cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB), \"Image loaded\"\n",
        "\n",
        "# --- Load Intrinsics ---\n",
        "def on_intrinsics_upload(file_obj):\n",
        "    if file_obj is None:\n",
        "        return \"No intrinsics file.\"\n",
        "    K, dist = load_intrinsics(file_obj.name)\n",
        "    CLICK_STATE[\"K\"] = K\n",
        "    CLICK_STATE[\"distCoeffs\"] = dist\n",
        "    return f\"Loaded K and distCoeffs...\\n\\nK = \\n{K}\\n\\ndistCoeffs = {dist}\"\n",
        "\n",
        "def draw_outlined_text(img, text, pos, font, font_size, font_color, text_size, outline_thickness):\n",
        "    outline_color = (255, 255, 255)\n",
        "    if sum(font_color) >= 255:\n",
        "        ouline_color = (0, 0, 0)\n",
        "    cv2.putText(img, text, pos, font, font_size, ouline_color, text_size + outline_thickness)\n",
        "    cv2.putText(img, text, pos, font, font_size, font_color, text_size)\n",
        "\n",
        "\n",
        "# --- Manual Click Handler ---\n",
        "def on_click(evt: gr.SelectData):\n",
        "    CLICK_STATE[\"clicks\"].append((evt.index[0], evt.index[1]))\n",
        "    vis = CLICK_STATE[\"display_img_bgr\"].copy()\n",
        "    for i, (x,y) in enumerate(CLICK_STATE[\"clicks\"]):\n",
        "        cv2.circle(vis, (int(x),int(y)), 10, (0,255,0), -1)\n",
        "        draw_outlined_text(vis, str(i+1), (int(x)+5,int(y)-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 3, 2)\n",
        "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB), f\"Clicks: {len(CLICK_STATE['clicks'])}\"\n",
        "\n",
        "# --- Auto-detect (stub: replace with your robust_auto_detect) ---\n",
        "def auto_detect_corners(cols, rows):\n",
        "    bgr = CLICK_STATE[\"img_bgr\"]\n",
        "    if bgr is None:\n",
        "        return None, \"No image loaded\"\n",
        "    # Fake detection for now: top-left 4 points\n",
        "    h, w = bgr.shape[:2]\n",
        "    corners = [(50,50),(150,50),(250,50),(350,50)]\n",
        "    CLICK_STATE[\"clicks\"] = corners\n",
        "    vis = bgr.copy()\n",
        "    for i,(x,y) in enumerate(corners):\n",
        "        cv2.circle(vis, (int(x),int(y)), 4, (0,255,0), -1)\n",
        "        draw_outlined_text(vis, str(i+1), (int(x)+5,int(y)-5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 3, 2)\n",
        "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB), f\"Auto: {len(corners)} points\"\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Pose from a Planar Object — Manual or Auto Points\")\n",
        "\n",
        "    with gr.Row():\n",
        "        img = gr.Image(type=\"numpy\", label=\"Upload planar scene\")\n",
        "        img_status = gr.Textbox(label=\"Image Status\", value=\"Awaiting image upload...\", interactive=False)\n",
        "        intr = gr.File(label=\"Upload intrinsics JSON\")\n",
        "        status_intr = gr.Textbox(label=\"Intrinsics status\",value=\"Awaiting JSON upload...\", interactive=False)\n",
        "        img.upload(on_image_upload, img, [img, img_status])\n",
        "        img.clear(lambda: \"Awaiting image upload...\", outputs=img_status)\n",
        "        intr.upload(on_intrinsics_upload, intr, status_intr)\n",
        "        intr.clear(lambda: \"Awaiting JSON upload...\", outputs=status_intr)\n",
        "\n",
        "    with gr.Row():\n",
        "        auto = gr.Button(\"Auto-detect Corners\")\n",
        "        clear = gr.Button(\"Clear Clicks\")\n",
        "\n",
        "    click_info = gr.Textbox(label=\"Click info\")\n",
        "\n",
        "    # manual clicks happen only here\n",
        "    img.select(on_click, None, [img, click_info])\n",
        "    auto.click(auto_detect_corners, [gr.Number(9), gr.Number(6)], [img, click_info])\n",
        "    clear.click(lambda: (CLICK_STATE[\"clicks\"].clear(), \"Clicks cleared\")[1], None, click_info)"
      ],
      "metadata": {
        "id": "ckC6nCsMZM2i"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(inline=True)"
      ],
      "metadata": {
        "id": "UDPBlupBkOXg",
        "outputId": "4967eade-5962-4cfd-fa4b-d3b853cdcc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d94a0a1acf52180f84.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d94a0a1acf52180f84.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Notes (Homography→Pose vs OpenCV)\n",
        "TODO FINISH THESE!!!!"
      ],
      "metadata": {
        "id": "qj9bqrMXbIBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- R matrices were (similar / slightly different); translation differs up to a common **scale** in homography path.\n",
        "- With undistorted image vs passing `distCoeffs`, results (were / were not) consistent; I chose (approach) for both paths.\n",
        "- With >4 points and mild noise, solvePnP was more stable; homography was sensitive to point order & spread across plane.\n",
        "- Enforcing `det(R)=+1` and SVD re-orthonormalization removed small drift from numeric noise.\n",
        "- Cheirality: not an issue with `solvePnP`; with `decomposeHomographyMat`, select solution with positive depths.\n",
        "- RANSAC for H improves robustness to a bad click but may drop to minimal 4 inliers → less stable pose."
      ],
      "metadata": {
        "id": "YtN8R4rMbP5W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIuA0ynrkV_X"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}